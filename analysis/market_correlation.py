import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Union
from datetime import datetime, timedelta
from data.db import DatabaseManager
from DMP.market_data_processor import MarketDataProcessor
from utils.logger import setup_logger
from utils.config import *
logger = setup_logger(__name__)


class MarketCorrelation:

    def __init__(self):

        self.db_manager = DatabaseManager()
        self.data_processor = MarketDataProcessor()

        logger.info("Ð†Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ð°Ð½Ð°Ð»Ñ–Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ñ€Ð¸Ð½ÐºÐ¾Ð²Ð¾Ñ— ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—")

        self.config = DEFAULT_CONFIG.copy()


    def _update_config_recursive(self, target_dict: Dict, source_dict: Dict) -> None:

        for key, value in source_dict.items():
            if key in target_dict and isinstance(target_dict[key], dict) and isinstance(value, dict):
                self._update_config_recursive(target_dict[key], value)
            else:
                logger.debug(f"ÐžÐ½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð° ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ—: {key} = {value}")
                target_dict[key] = value

    def calculate_price_correlation(self, symbols: List[str],
                                    timeframe: str = None,
                                    start_time: Optional[datetime] = None,
                                    end_time: Optional[datetime] = None,
                                    method: str = None) -> pd.DataFrame:

        # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼ Ð· ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ—, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ñ–
        timeframe = self.config['default_timeframe']
        method = self.config['default_correlation_method']

        logger.info(f"Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ñ†Ñ–Ð½ Ð´Ð»Ñ {len(symbols)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð¾Ð¼ {timeframe}")

        # Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ ÐºÑ–Ð½Ñ†ÐµÐ²Ð¾Ð³Ð¾ Ñ‡Ð°ÑÑƒ ÑÐº Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ð¸Ð¹, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾
        end_time = end_time or datetime.now()

        # Ð¯ÐºÑ‰Ð¾ Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÐ¾Ð²Ð¸Ð¹ Ñ‡Ð°Ñ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾, Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼
        if start_time is None:
            lookback_days = self.config['default_lookback_days']
            start_time = end_time - timedelta(days=lookback_days)
            logger.debug(f"Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð¿ÐµÑ€Ñ–Ð¾Ð´Ñƒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼: {lookback_days} Ð´Ð½Ñ–Ð²")

        try:
            # ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ñ†Ñ–Ð½Ð¾Ð²Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ð±Ð°Ð·Ð¸ Ð´Ð°Ð½Ð¸Ñ… Ð·Ð°Ð¼Ñ–ÑÑ‚ÑŒ Binance API
            price_data = {}
            for symbol in symbols:
                logger.debug(f"ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ {symbol}")
                # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ db_manager.get_klines Ð·Ð°Ð¼Ñ–ÑÑ‚ÑŒ binance_client.get_historical_prices
                price_data[symbol] = self.db_manager.get_klines(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_time=start_time,
                    end_time=end_time
                )

            # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼Ñƒ Ð· Ñ†Ñ–Ð½Ð°Ð¼Ð¸ Ð·Ð°ÐºÑ€Ð¸Ñ‚Ñ‚Ñ Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð²
            df = pd.DataFrame()
            for symbol, data in price_data.items():
                data['open_time'] = pd.to_datetime(data['open_time'], unit='s')  # Ð°Ð±Ð¾ unit='ms' â€” Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                data.set_index('open_time', inplace=True)
                df[symbol] = data['close']

            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ– Ð´Ð°Ð½Ñ–
            if df.isnull().values.any():
                missing_count = df.isnull().sum().sum()
                logger.warning(f"Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð¾ {missing_count} Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ–Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ. Ð—Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ð½Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ forward fill")
                df = df.fillna(method='ffill')

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ– ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—
            correlation_matrix = df.corr(method=method)

            logger.info(f"ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ñ†Ñ–Ð½ ÑƒÑÐ¿Ñ–ÑˆÐ½Ð¾ Ñ€Ð¾Ð·Ñ€Ð°Ñ…Ð¾Ð²Ð°Ð½Ð° Ð· Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½ÑÐ¼ Ð¼ÐµÑ‚Ð¾Ð´Ñƒ {method}")

            # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð² Ñƒ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð¸Ñ…
            self.save_correlation_to_db(
                correlation_matrix=correlation_matrix,
                correlation_type='price',
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                method=method
            )

            return correlation_matrix

        except Exception as e:
            logger.error(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½ÐºÑƒ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ñ†Ñ–Ð½: {str(e)}")
            raise

    def calculate_volume_correlation(self, symbols: List[str],
                                     timeframe: str = None,
                                     start_time: Optional[datetime] = None,
                                     end_time: Optional[datetime] = None,
                                     method: str = None) -> pd.DataFrame:

        # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼ Ð· ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ—, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ñ–
        timeframe = timeframe or self.config['default_timeframe']
        method = method or self.config['default_correlation_method']

        logger.info(f"Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð¾Ð±'Ñ”Ð¼Ñ–Ð² Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ– Ð´Ð»Ñ {len(symbols)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð¾Ð¼ {timeframe}")

        # Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ ÐºÑ–Ð½Ñ†ÐµÐ²Ð¾Ð³Ð¾ Ñ‡Ð°ÑÑƒ ÑÐº Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ð¸Ð¹, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾
        end_time = end_time or datetime.now()

        # Ð¯ÐºÑ‰Ð¾ Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÐ¾Ð²Ð¸Ð¹ Ñ‡Ð°Ñ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾, Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼
        if start_time is None:
            lookback_days = self.config['default_lookback_days']
            start_time = end_time - timedelta(days=lookback_days)
            logger.debug(f"Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð¿ÐµÑ€Ñ–Ð¾Ð´Ñƒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼: {lookback_days} Ð´Ð½Ñ–Ð²")

        try:
            # ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð¿Ñ€Ð¾ Ð¾Ð±'Ñ”Ð¼Ð¸ Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ– Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ð±Ð°Ð·Ð¸ Ð´Ð°Ð½Ð¸Ñ…
            volume_data = {}
            for symbol in symbols:
                logger.debug(f"ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð¿Ñ€Ð¾ Ð¾Ð±'Ñ”Ð¼Ð¸ Ð´Ð»Ñ {symbol}")
                volume_data[symbol] = self.db_manager.get_klines(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_time=start_time,
                    end_time=end_time
                )

            # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼Ñƒ Ð· Ð¾Ð±'Ñ”Ð¼Ð°Ð¼Ð¸ Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ– Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð²
            df = pd.DataFrame()
            for symbol, data in volume_data.items():
                # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ñ–Ñ Ð´Ð¾ float Ð²Ñ–Ð´Ñ€Ð°Ð·Ñƒ Ð¿Ñ–Ð´ Ñ‡Ð°Ñ ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼Ñƒ
                df[symbol] = data['volume'].astype(float)

            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ– Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ
            if df.isnull().values.any():
                missing_count = df.isnull().sum().sum()
                logger.warning(f"Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð¾ {missing_count} Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ–Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ Ð¾Ð±'Ñ”Ð¼Ñƒ. Ð—Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ð½Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ forward fill")
                df = df.fillna(method='ffill')

            # Ð¤Ñ–Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ñ–Ñ Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ ÑƒÑÑƒÐ½ÐµÐ½Ð½Ñ Ð²Ð¸ÐºÐ¸Ð´Ñ–Ð²
            for column in df.columns:
                mean_val = df[column].mean()
                std_val = df[column].std()
                upper_limit = mean_val + 3 * std_val

                # Ð—Ð°Ð¼Ñ–Ð½Ð° Ð·Ð½Ð°Ñ‡Ð½Ð¸Ñ… Ð²Ð¸ÐºÐ¸Ð´Ñ–Ð² ÑÐµÑ€ÐµÐ´Ð½Ñ–Ð¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½ÑÐ¼
                outliers_mask = df[column] > upper_limit
                if outliers_mask.any():
                    outlier_count = outliers_mask.sum()
                    logger.warning(
                        f"Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð¾ {outlier_count} Ð²Ð¸ÐºÐ¸Ð´Ñ–Ð² Ð¾Ð±'Ñ”Ð¼Ñƒ Ð´Ð»Ñ {column}. Ð—Ð°Ð¼Ñ–Ð½Ð° Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½ÑÐ¼Ð¸ Ð·Ð° Ð¼ÐµÐ´Ñ–Ð°Ð½Ð¾ÑŽ")
                    df.loc[outliers_mask, column] = df[column].median()

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ– ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—
            correlation_matrix = df.corr(method=method)

            logger.info(f"ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð¾Ð±'Ñ”Ð¼Ñ–Ð² Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ– ÑƒÑÐ¿Ñ–ÑˆÐ½Ð¾ Ñ€Ð¾Ð·Ñ€Ð°Ñ…Ð¾Ð²Ð°Ð½Ð° Ð· Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½ÑÐ¼ Ð¼ÐµÑ‚Ð¾Ð´Ñƒ {method}")

            # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð² Ñƒ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð¸Ñ…
            self.save_correlation_to_db(
                correlation_matrix=correlation_matrix,
                correlation_type='volume',
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                method=method
            )

            return correlation_matrix

        except Exception as e:
            logger.error(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½ÐºÑƒ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð¾Ð±'Ñ”Ð¼Ñ–Ð² Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ–: {str(e)}")
            raise

    def calculate_returns_correlation(self, symbols: List[str],
                                      timeframe: str = None,
                                      start_time: Optional[datetime] = None,
                                      end_time: Optional[datetime] = None,
                                      period: int = 1,
                                      method: str = None) -> pd.DataFrame:

        # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼ Ð· ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ—, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ñ–
        timeframe = timeframe or self.config['default_timeframe']
        method = method or self.config['default_correlation_method']

        logger.info(
            f"Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ– Ð´Ð»Ñ {len(symbols)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð¾Ð¼ {timeframe} Ñ‚Ð° Ð¿ÐµÑ€Ñ–Ð¾Ð´Ð¾Ð¼ {period}")

        # Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ ÐºÑ–Ð½Ñ†ÐµÐ²Ð¾Ð³Ð¾ Ñ‡Ð°ÑÑƒ ÑÐº Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ð¸Ð¹, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾
        end_time = end_time or datetime.now()

        # Ð¯ÐºÑ‰Ð¾ Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÐ¾Ð²Ð¸Ð¹ Ñ‡Ð°Ñ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾, Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼
        if start_time is None:
            lookback_days = self.config['default_lookback_days']
            start_time = end_time - timedelta(days=lookback_days)
            logger.debug(f"Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð¿ÐµÑ€Ñ–Ð¾Ð´Ñƒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼: {lookback_days} Ð´Ð½Ñ–Ð²")

        try:
            # ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ñ†Ñ–Ð½Ð¾Ð²Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ð±Ð°Ð·Ð¸ Ð´Ð°Ð½Ð¸Ñ…
            price_data = {}
            for symbol in symbols:
                logger.debug(f"ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ {symbol}")
                # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ db_manager.get_klines Ð·Ð°Ð¼Ñ–ÑÑ‚ÑŒ binance_client.get_historical_prices
                price_data[symbol] = self.db_manager.get_klines(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_time=start_time,
                    end_time=end_time
                )

            # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼Ñƒ Ð· Ñ†Ñ–Ð½Ð°Ð¼Ð¸ Ð·Ð°ÐºÑ€Ð¸Ñ‚Ñ‚Ñ Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð²
            df = pd.DataFrame()
            for symbol, data in price_data.items():
                data['open_time'] = pd.to_datetime(data['open_time'], unit='s')  # Ð°Ð±Ð¾ unit='ms' â€” Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                data.set_index('open_time', inplace=True)
                df[symbol] = data['close']

            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ– Ð´Ð°Ð½Ñ–
            if df.isnull().values.any():
                missing_count = df.isnull().sum().sum()
                logger.warning(f"Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð¾ {missing_count} Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ–Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ. Ð—Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ð½Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ forward fill")
                df = df.fillna(method='ffill')

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–
            returns_df = df.pct_change(period)

            # Ð’Ð¸Ð´Ð°Ð»ÐµÐ½Ð½Ñ Ð¿ÐµÑ€ÑˆÐ¸Ñ… Ñ€ÑÐ´ÐºÑ–Ð², ÑÐºÑ– Ð¼Ñ–ÑÑ‚ÑÑ‚ÑŒ NaN Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–
            returns_df = returns_df.iloc[period:]

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ– ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–
            correlation_matrix = returns_df.corr(method=method)

            logger.info(f"ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ– ÑƒÑÐ¿Ñ–ÑˆÐ½Ð¾ Ñ€Ð¾Ð·Ñ€Ð°Ñ…Ð¾Ð²Ð°Ð½Ð° Ð· Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½ÑÐ¼ Ð¼ÐµÑ‚Ð¾Ð´Ñƒ {method}")

            # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð² Ñƒ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð¸Ñ…
            self.save_correlation_to_db(
                correlation_matrix=correlation_matrix,
                correlation_type='returns',
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                method=method
            )

            return correlation_matrix

        except Exception as e:
            logger.error(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½ÐºÑƒ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–: {str(e)}")
            raise

    def calculate_volatility_correlation(self, symbols: List[str],
                                         timeframe: str = None,
                                         start_time: Optional[datetime] = None,
                                         end_time: Optional[datetime] = None,
                                         window: int = None,
                                         method: str = None) -> pd.DataFrame:

        # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼ Ð· ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ—, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ñ–
        timeframe = timeframe or self.config['default_timeframe']
        window = window or self.config['default_correlation_window']
        method = method or self.config['default_correlation_method']

        logger.info(
            f"Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ– Ð´Ð»Ñ {len(symbols)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð¾Ð¼ {timeframe} Ñ‚Ð° Ð²Ñ–ÐºÐ½Ð¾Ð¼ {window}")

        # Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ ÐºÑ–Ð½Ñ†ÐµÐ²Ð¾Ð³Ð¾ Ñ‡Ð°ÑÑƒ ÑÐº Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ð¸Ð¹, ÑÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾
        end_time = end_time or datetime.now()

        # Ð¯ÐºÑ‰Ð¾ Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÐ¾Ð²Ð¸Ð¹ Ñ‡Ð°Ñ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾, Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼
        if start_time is None:
            lookback_days = self.config['default_lookback_days']
            start_time = end_time - timedelta(days=lookback_days)
            logger.debug(f"Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð¿ÐµÑ€Ñ–Ð¾Ð´Ñƒ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼: {lookback_days} Ð´Ð½Ñ–Ð²")

        try:
            # ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ñ†Ñ–Ð½Ð¾Ð²Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð² Ð· Ð±Ð°Ð·Ð¸ Ð´Ð°Ð½Ð¸Ñ…
            price_data = {}
            for symbol in symbols:
                logger.debug(f"ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ {symbol}")
                # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ db_manager.get_klines Ð·Ð°Ð¼Ñ–ÑÑ‚ÑŒ binance_client.get_historical_prices
                price_data[symbol] = self.db_manager.get_klines(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_time=start_time,
                    end_time=end_time
                )

            # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼Ñƒ Ð· Ñ†Ñ–Ð½Ð°Ð¼Ð¸ Ð·Ð°ÐºÑ€Ð¸Ñ‚Ñ‚Ñ Ð´Ð»Ñ Ð²ÑÑ–Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ–Ð²
            price_df = pd.DataFrame()
            for symbol, data in price_data.items():
                data['open_time'] = pd.to_datetime(data['open_time'], unit='s')  # Ð°Ð±Ð¾ unit='ms' â€” Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                data.set_index('open_time', inplace=True)
                price_df[symbol] = data['close']

            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ– Ð´Ð°Ð½Ñ–
            if price_df.isnull().values.any():
                missing_count = price_df.isnull().sum().sum()
                logger.warning(f"Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð¾ {missing_count} Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ–Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ. Ð—Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ð½Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ forward fill")
                price_df = price_df.fillna(method='ffill')

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–
            returns_df = price_df.pct_change()
            returns_df = returns_df.iloc[1:]  # Ð’Ð¸Ð´Ð°Ð»ÐµÐ½Ð½Ñ Ð¿ÐµÑ€ÑˆÐ¾Ð³Ð¾ Ñ€ÑÐ´ÐºÐ°, ÑÐºÐ¸Ð¹ Ð¼Ñ–ÑÑ‚Ð¸Ñ‚ÑŒ NaN

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ– (ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ðµ Ð²Ñ–Ð´Ñ…Ð¸Ð»ÐµÐ½Ð½Ñ Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–)
            volatility_df = pd.DataFrame()
            for symbol in symbols:
                volatility_df[symbol] = returns_df[symbol].rolling(window=window).std()

            # Ð’Ð¸Ð´Ð°Ð»ÐµÐ½Ð½Ñ Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÐ¾Ð²Ð¸Ñ… Ñ€ÑÐ´ÐºÑ–Ð², ÑÐºÑ– Ð¼Ñ–ÑÑ‚ÑÑ‚ÑŒ NaN Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ ÐºÐ¾Ð²Ð·Ð½Ð¾Ð³Ð¾ Ð²Ñ–ÐºÐ½Ð°
            volatility_df = volatility_df.iloc[window - 1:]

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ– ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–
            correlation_matrix = volatility_df.corr(method=method)

            logger.info(f"ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ– ÑƒÑÐ¿Ñ–ÑˆÐ½Ð¾ Ñ€Ð¾Ð·Ñ€Ð°Ñ…Ð¾Ð²Ð°Ð½Ð° Ð· Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½ÑÐ¼ Ð¼ÐµÑ‚Ð¾Ð´Ñƒ {method}")

            # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð² Ñƒ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð¸Ñ…
            self.save_correlation_to_db(
                correlation_matrix=correlation_matrix,
                correlation_type='volatility',
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                method=method
            )

            return correlation_matrix

        except Exception as e:
            logger.error(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½ÐºÑƒ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–: {str(e)}")
            raise

    def get_correlated_pairs(self, correlation_matrix: pd.DataFrame,
                             threshold: float = None) -> List[Tuple[str, str, float]]:

        # Use default threshold from config if not specified
        threshold = threshold or self.config['correlation_threshold']

        # Initialize results list
        correlated_pairs = []

        # Get the symbols (assuming they are both column and index names)
        symbols = correlation_matrix.columns.tolist()

        # Iterate through upper triangle of correlation matrix
        for i in range(len(symbols)):
            for j in range(i + 1, len(symbols)):  # Start from i+1 to avoid duplicates and self-correlations
                symbol1, symbol2 = symbols[i], symbols[j]
                correlation = correlation_matrix.loc[symbol1, symbol2]

                # Check if correlation is above threshold
                if correlation >= threshold:
                    correlated_pairs.append((symbol1, symbol2, correlation))

        # Sort pairs by correlation value in descending order
        correlated_pairs.sort(key=lambda x: x[2], reverse=True)

        logger.info(f"Found {len(correlated_pairs)} highly correlated pairs with threshold {threshold}")
        return correlated_pairs

    def get_anticorrelated_pairs(self, correlation_matrix: pd.DataFrame,
                                 threshold: float = None) -> List[Tuple[str, str, float]]:

        # Use default threshold from config if not specified
        threshold = threshold or self.config['anticorrelation_threshold']

        # Initialize results list
        anticorrelated_pairs = []

        # Get the symbols (assuming they are both column and index names)
        symbols = correlation_matrix.columns.tolist()

        # Iterate through upper triangle of correlation matrix
        for i in range(len(symbols)):
            for j in range(i + 1, len(symbols)):  # Start from i+1 to avoid duplicates and self-correlations
                symbol1, symbol2 = symbols[i], symbols[j]
                correlation = correlation_matrix.loc[symbol1, symbol2]

                # Check if correlation is below threshold (negative correlation)
                if correlation <= threshold:
                    anticorrelated_pairs.append((symbol1, symbol2, correlation))

        # Sort pairs by correlation value in ascending order (most negative first)
        anticorrelated_pairs.sort(key=lambda x: x[2])

        logger.info(f"Found {len(anticorrelated_pairs)} highly anti-correlated pairs with threshold {threshold}")
        return anticorrelated_pairs

    def calculate_rolling_correlation(self, symbol1: str, symbol2: str,
                                      timeframe: str = None,
                                      start_time: Optional[datetime] = None,
                                      end_time: Optional[datetime] = None,
                                      window: int = None,
                                      method: str = None) -> pd.Series:
        # Use default values from config if not specified
        timeframe = timeframe or self.config['default_timeframe']
        window = window or self.config['default_correlation_window']
        method = method or self.config['default_correlation_method']

        # Set default time range if not specified
        if end_time is None:
            end_time = datetime.now()
        if start_time is None:
            # Add extra data to accommodate the window size
            start_time = end_time - timedelta(days=window + 30)  # +30 days buffer

        logger.info(f"Calculating rolling correlation between {symbol1} and {symbol2} with window={window}")

        try:
            # Fetch price data for both symbols
            df1 = self.db_manager.get_klines(
                symbol=symbol1,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time
            )

            df2 = self.db_manager.get_klines(
                symbol=symbol2,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time
            )

            # Extract close prices
            price1 = df1['close']
            price2 = df2['close']

            # Align the time series (handle missing data points)
            aligned_prices = pd.DataFrame({
                symbol1: price1,
                symbol2: price2
            })

            # Calculate percent changes (returns)
            returns = aligned_prices.pct_change().dropna()

            # Calculate rolling correlation (method='pearson' only)
            rolling_corr = returns[symbol1].rolling(window=window).corr(returns[symbol2])

            logger.info(f"Successfully calculated rolling correlation with {len(rolling_corr)} data points")
            return rolling_corr

        except Exception as e:
            logger.error(f"Error calculating rolling correlation: {str(e)}")
            raise

    def detect_correlation_breakdowns(self, symbol1: str, symbol2: str,
                                      timeframe: str = None,
                                      start_time: Optional[datetime] = None,
                                      end_time: Optional[datetime] = None,
                                      window: int = None,
                                      threshold: float = None) -> List[datetime]:

        # Use default values from config if not specified
        timeframe = timeframe or self.config['default_timeframe']
        window = window or self.config['default_correlation_window']
        threshold = threshold or self.config['breakdown_threshold']

        logger.info(f"Detecting correlation breakdowns between {symbol1} and {symbol2} with threshold {threshold}")

        try:
            # Calculate rolling correlation between the two symbols
            rolling_corr = self.calculate_rolling_correlation(
                symbol1=symbol1,
                symbol2=symbol2,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                window=window
            )

            # Calculate absolute changes in correlation
            correlation_changes = rolling_corr.diff().abs()

            # Identify points where correlation changes more than the threshold
            breakdown_points = correlation_changes[correlation_changes > threshold].index.tolist()

            logger.info(f"Found {len(breakdown_points)} correlation breakdown points")

            # Save breakdown data to database
            breakdown_data = []
            for point in breakdown_points:
                correlation_before = rolling_corr.loc[rolling_corr.index < point].iloc[-1] if not rolling_corr.loc[
                    rolling_corr.index < point].empty else None
                correlation_after = rolling_corr.loc[point]
                change_magnitude = correlation_changes.loc[point]

                breakdown_data.append({
                    'timestamp': pd.to_datetime(point).to_pydatetime(),  # ðŸ”§ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ñ–Ñ Ð´Ð¾ datetime.datetime
                    'symbol1': symbol1,
                    'symbol2': symbol2,
                    'correlation_before': correlation_before,
                    'correlation_after': correlation_after,
                    'change_magnitude': change_magnitude
                })

            # Save to correlation_breakdowns table
            if breakdown_data:
                for point_data in breakdown_data:
                    self.db_manager.save_correlation_breakdown(
                        breakdown_time=point_data['timestamp'],
                        symbol1=point_data['symbol1'],
                        symbol2=point_data['symbol2'],
                        correlation_before=point_data['correlation_before'],
                        correlation_after=point_data['correlation_after'],
                        method=self.config.get('default_correlation_method', 'pearson'),
                        threshold=threshold,
                        timeframe=timeframe,
                        window_size=window
                    )
                logger.debug(f"Saved {len(breakdown_data)} breakdown points to database")

            return breakdown_points

        except Exception as e:
            logger.error(f"Error detecting correlation breakdowns: {str(e)}")
            raise

    def calculate_market_beta(self, symbol: str, market_symbol: str = 'BTCUSDT',
                              timeframe: str = None,
                              start_time: Optional[datetime] = None,
                              end_time: Optional[datetime] = None,
                              window: int = None) -> Union[float, pd.Series]:

        # Use default values from config if not specified
        timeframe = timeframe or self.config['default_timeframe']
        window = window or self.config['default_correlation_window']

        logger.info(f"Calculating market beta for {symbol} relative to {market_symbol}")

        # Set default time range if not specified
        end_time = end_time or datetime.now()
        if start_time is None:
            lookback_days = self.config['default_lookback_days']
            start_time = end_time - timedelta(days=lookback_days)

        try:
            # Fetch price data for asset and market benchmark
            asset_data = self.db_manager.get_klines(
                symbol=symbol,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time
            )

            market_data = self.db_manager.get_klines(
                symbol=market_symbol,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time
            )

            # Calculate returns and convert to float immediately
            asset_prices = asset_data['close'].astype(float)
            market_prices = market_data['close'].astype(float)

            # Align time series
            df = pd.DataFrame({
                'asset': asset_prices,
                'market': market_prices
            })

            # Calculate returns
            returns = df.pct_change().dropna()

            # If window is specified, calculate rolling beta
            if window:
                # Calculate rolling covariance and market variance
                rolling_cov = returns['asset'].rolling(window=window).cov(returns['market'])
                rolling_market_var = returns['market'].rolling(window=window).var()

                # Calculate rolling beta
                rolling_beta = rolling_cov / rolling_market_var

                # Save beta time series to database
                beta_records = []
                for timestamp, beta_value in rolling_beta.items():
                    if not np.isnan(beta_value) and not np.isinf(beta_value):
                        beta_records.append({
                            'timestamp': timestamp,
                            'symbol': symbol,
                            'market_symbol': market_symbol,
                            'beta': float(beta_value),  # Ensure beta is float
                            'timeframe': timeframe,
                            'window': window
                        })

                # Save to beta_time_series table
                if beta_records:
                    timestamps = [record['timestamp'] for record in beta_records]
                    beta_values = [record['beta'] for record in beta_records]

                    # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚ÑƒÑ”Ð¼Ð¾ pd.Timestamp Ñƒ datetime.datetime
                    timestamps = [ts.to_pydatetime() if isinstance(ts, pd.Timestamp) else ts for ts in timestamps]

                    self.db_manager.save_beta_time_series(
                        symbol=symbol,
                        market_symbol=market_symbol,
                        timestamps=timestamps,
                        beta_values=beta_values,
                        timeframe=timeframe,
                        window_size=window
                    )

                    logger.debug(f"Saved {len(beta_records)} beta values to database")

                return rolling_beta
            else:
                # Calculate overall beta
                beta = np.cov(returns['asset'], returns['market'])[0, 1] / np.var(returns['market'])

                # Save to market_betas table
                beta_record = {
                    'symbol': symbol,
                    'market_symbol': market_symbol,
                    'beta': float(beta),  # Ensure beta is float
                    'timeframe': timeframe,
                    'start_time': start_time,
                    'end_time': end_time,
                }

                self.db_manager.save_market_beta([beta_record])
                logger.info(f"Saved beta value {beta} for {symbol} to database")

                return beta

        except Exception as e:
            logger.error(f"Error calculating market beta for {symbol}: {str(e)}")
            raise


    def save_correlation_to_db(self, correlation_matrix: pd.DataFrame,
                               correlation_type: str,
                               timeframe: str,
                               start_time: datetime,
                               end_time: datetime,
                               method: str) -> bool:

        # Use default method from config if not specified
        method = method or self.config['default_correlation_method']

        logger.info(f"Saving {correlation_type} correlation matrix to database")

        try:
            # Create a record for the correlation matrix
            matrix_id = f"{correlation_type}_{timeframe}_{start_time.strftime('%Y%m%d')}_{end_time.strftime('%Y%m%d')}_{method}"

            # Prepare data for the correlation_matrices table
            matrix_data = {
                'matrix_id': matrix_id,
                'correlation_type': correlation_type,
                'timeframe': timeframe,
                'start_time': start_time,
                'end_time': end_time,
                'method': method,
                'analysis_time': datetime.now(),
                'symbols': correlation_matrix.columns.tolist(),
                'matrix_json': correlation_matrix.to_json()
            }

            # Save to correlation_matrices table
            self.db_manager.save_correlation_matrix(
                correlation_matrix.columns.tolist(),
                correlation_type,
                timeframe,
                start_time,
                end_time=end_time,
                method='pearson'
            )
            # Extract and save highly correlated pairs
            symbols = correlation_matrix.columns.tolist()
            pairs_data = []

            for i in range(len(symbols)):
                for j in range(i + 1, len(symbols)):
                    symbol1, symbol2 = symbols[i], symbols[j]
                    correlation = correlation_matrix.loc[symbol1, symbol2]

                    if abs(correlation) >= self.config['correlation_threshold']:
                        pair_data = {
                            'matrix_id': matrix_id,
                            'symbol1': symbol1,
                            'symbol2': symbol2,
                            'correlation_value': correlation,  # Changed from 'correlation'
                            'correlation_type': correlation_type,
                            'timeframe': timeframe,
                            'start_time': start_time,  # Added
                            'end_time': end_time,  # Added
                            'method': method,  # Added
                            'analysis_time': datetime.now()
                        }
                        pairs_data.append(pair_data)

            # Save to correlated_pairs table
            if pairs_data:
                self.db_manager.insert_correlated_pair(pairs_data,method='pearson')
                logger.debug(f"Saved {len(pairs_data)} correlated pairs to database")

            logger.info(f"Successfully saved correlation matrix with ID {matrix_id}")
            return True

        except Exception as e:
            logger.error(f"Error saving correlation to database: {str(e)}")
            return False

    def load_correlation_from_db(self, correlation_type: str,
                                 timeframe: str,
                                 start_time: datetime,
                                 end_time: datetime,
                                 method: str = None) -> Optional[pd.DataFrame]:

        # Use default method from config if not specified
        method = method or self.config['default_correlation_method']

        logger.info(f"Loading {correlation_type} correlation matrix from database")

        try:
            # Create the matrix ID for lookup
            matrix_id = f"{correlation_type}_{timeframe}_{start_time.strftime('%Y%m%d')}_{end_time.strftime('%Y%m%d')}_{method}"

            # Load from correlation_matrices table
            matrix_data = self.db_manager.get_correlation_matrix(matrix_id)

            if matrix_data:
                # Convert JSON string back to DataFrame
                matrix_json = matrix_data.get('matrix_json')
                if matrix_json:
                    correlation_matrix = pd.read_json(matrix_json)
                    logger.info(f"Successfully loaded correlation matrix with ID {matrix_id}")
                    return correlation_matrix

            logger.warning(f"No correlation matrix found with ID {matrix_id}")
            return None

        except Exception as e:
            logger.error(f"Error loading correlation from database: {str(e)}")
            return None

    def correlation_time_series(self, symbols_pair: Tuple[str, str],
                                correlation_window: int = None,
                                lookback_days: int = None,
                                timeframe: str = None) -> pd.Series:

        # Use default values from config if not specified
        correlation_window = correlation_window or self.config['default_correlation_window']
        lookback_days = lookback_days or self.config['default_lookback_days']
        timeframe = timeframe or self.config['default_timeframe']

        symbol1, symbol2 = symbols_pair
        logger.info(f"Calculating correlation time series between {symbol1} and {symbol2}")

        # Set time range
        end_time = datetime.now()
        start_time = end_time - timedelta(days=lookback_days)

        try:
            # First check if we already have this data in the database
            correlation_series = self.db_manager.get_correlation_time_series(
                symbol1=symbol1,
                symbol2=symbol2,
                start_time=start_time,
                end_time=end_time,
                timeframe=timeframe,
                window=correlation_window
            )

            if correlation_series is not None and not correlation_series.empty:
                logger.info(f"Found existing correlation time series in database")
                return correlation_series

            # If not found in database, calculate it
            logger.info(f"Calculating new correlation time series")

            # Calculate rolling correlation
            rolling_corr = self.calculate_rolling_correlation(
                symbol1=symbol1,
                symbol2=symbol2,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                window=correlation_window
            )

            # Save correlation time series to database
            correlation_data = []
            for timestamp, corr_value in rolling_corr.items():
                if not np.isnan(corr_value):
                    correlation_data.append({
                        'timestamp': timestamp,
                        'symbol1': symbol1,
                        'symbol2': symbol2,
                        'correlation': corr_value,
                        'timeframe': timeframe,
                        'window': correlation_window
                    })

            # Save to correlation_time_series table
            if correlation_data:
                self.db_manager.save_correlation_time_series(correlation_data,method='pearson')
                logger.debug(f"Saved {len(correlation_data)} correlation values to database")

            return rolling_corr

        except Exception as e:
            logger.error(f"Error calculating correlation time series: {str(e)}")
            raise

    def find_leading_indicators(self, target_symbol: str,
                                candidate_symbols: List[str],
                                lag_periods: List[int] = None,
                                timeframe: str = None,
                                start_time: Optional[datetime] = None,
                                end_time: Optional[datetime] = None) -> Dict[str, Dict[int, float]]:

        # Use default values from config if not specified
        lag_periods = lag_periods or self.config['default_lag_periods']
        timeframe = timeframe or self.config['default_timeframe']

        logger.info(f"Finding leading indicators for {target_symbol} among {len(candidate_symbols)} candidates")

        # Set default time range if not specified
        end_time = end_time or datetime.now()
        if start_time is None:
            lookback_days = self.config['default_lookback_days']
            start_time = end_time - timedelta(days=lookback_days)

        try:
            # Fetch target data
            target_data = self.db_manager.get_klines(
                symbol=target_symbol,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time
            )

            # Calculate target returns
            target_prices = target_data['close']
            target_returns = target_prices.pct_change().dropna()

            # Dictionary to store results
            leading_indicators = {}
            leading_indicators_records = []

            # Test each candidate symbol
            for symbol in candidate_symbols:
                if symbol == target_symbol:
                    continue

                logger.debug(f"Testing {symbol} as a leading indicator")

                # Fetch candidate data
                candidate_data = self.db_manager.get_klines(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_time=start_time,
                    end_time=end_time
                )

                # Calculate candidate returns
                candidate_prices = candidate_data['close']
                candidate_returns = candidate_prices.pct_change().dropna()

                # Align time series
                common_index = target_returns.index.intersection(candidate_returns.index)
                if len(common_index) < max(lag_periods) + 10:  # Need enough data points
                    logger.warning(f"Insufficient aligned data points for {symbol}")
                    continue

                aligned_target = target_returns.loc[common_index]
                aligned_candidate = candidate_returns.loc[common_index]

                # Calculate lagged correlations
                lag_correlations = {}
                for lag in lag_periods:
                    # Shift candidate data forward to test if it leads target
                    lagged_candidate = aligned_candidate.shift(lag)

                    # Remove NaN values created by shifting
                    valid_indices = aligned_target.index.intersection(lagged_candidate.dropna().index)
                    if len(valid_indices) < 10:  # Need enough data points for correlation
                        continue

                    # Calculate correlation between target and lagged candidate
                    correlation = aligned_target.loc[valid_indices].corr(lagged_candidate.loc[valid_indices])
                    lag_correlations[lag] = correlation

                    # Add to records for database
                    leading_indicators_records.append({
                        'target_symbol': target_symbol,
                        'indicator_symbol': symbol,
                        'lag_period': lag,
                        'correlation_value': correlation,
                        'timeframe': timeframe,
                        'start_time': start_time,
                        'end_time': end_time,
                        'analysis_time': datetime.now(),
                        'method': 'pearson'
                    })

                # Only add to results if at least one lag period had a correlation
                if lag_correlations:
                    leading_indicators[symbol] = lag_correlations

            # Save to leading_indicators table
            if leading_indicators_records:
                self.db_manager.save_leading_indicators(leading_indicators_records)
                logger.debug(f"Saved {len(leading_indicators_records)} leading indicator records to database")

            return leading_indicators

        except Exception as e:
            logger.error(f"Error finding leading indicators: {str(e)}")
            raise

    def correlated_movement_prediction(self, symbol: str,
                                       correlated_symbols: List[str],
                                       prediction_horizon: int = None,
                                       timeframe: str = None) -> Dict[str, float]:

        # Use default values from config if not specified
        prediction_horizon = prediction_horizon or self.config['default_prediction_horizon']
        timeframe = timeframe or self.config['default_timeframe']

        logger.info(
            f"Predicting movement for {symbol} using {len(correlated_symbols)} correlated symbols with horizon {prediction_horizon}")

        # Set default time range - we need more historical data for training
        end_time = datetime.now()
        lookback_days = self.config['default_lookback_days'] * 2  # Double the usual lookback for better training
        start_time = end_time - timedelta(days=lookback_days)

        try:
            # Fetch price data for target and all correlated symbols
            all_symbols = [symbol] + correlated_symbols
            price_data = {}
            for sym in all_symbols:
                price_data[sym] = self.db_manager.get_klines(
                    symbol=sym,
                    timeframe=timeframe,
                    start_time=start_time,
                    end_time=end_time
                )

            # Extract close prices and convert to float
            close_prices = {}
            for sym, data in price_data.items():
                # Convert Decimal to float if needed
                close_prices[sym] = pd.Series(data['close']).astype(float)

            # Calculate returns
            returns = {}
            for sym, prices in close_prices.items():
                returns[sym] = prices.pct_change().dropna()

            # Create a dataframe with all returns
            all_returns = pd.DataFrame({sym: ret for sym, ret in returns.items()})

            # Handle any missing data
            if all_returns.isnull().values.any():
                missing_count = all_returns.isnull().sum().sum()
                logger.warning(f"Found {missing_count} missing values in returns. Filling with forward fill.")
                all_returns = all_returns.fillna(method='ffill')

            # Create prediction features by shifting correlated symbols' returns
            feature_df = pd.DataFrame()
            for i, corr_sym in enumerate(correlated_symbols):
                for lag in range(1, prediction_horizon + 1):
                    feature_name = f"{corr_sym}_lag_{lag}"
                    feature_df[feature_name] = all_returns[corr_sym].shift(lag)

            # Target is future return of the symbol we're trying to predict
            target = all_returns[symbol].shift(-prediction_horizon)

            # Combine features and target
            model_data = pd.concat([feature_df, target], axis=1)
            model_data.columns = list(feature_df.columns) + ['target']

            # Remove rows with NaN values
            model_data = model_data.dropna()

            if len(model_data) < 30:  # Need sufficient data for reliable prediction
                logger.warning(f"Insufficient data for prediction after preprocessing: {len(model_data)} rows")
                return {"error": "Insufficient data for prediction"}

            # Split data into training and testing sets
            train_size = int(len(model_data) * 0.8)
            train_data = model_data.iloc[:train_size]
            test_data = model_data.iloc[train_size:]

            # Train a simple linear regression model
            from sklearn.linear_model import LinearRegression

            # Ensure all data is float type
            X_train = train_data.drop('target', axis=1).astype(float)
            y_train = train_data['target'].astype(float)

            model = LinearRegression()
            model.fit(X_train, y_train)

            # Evaluate on test set - ensure test data is also float type
            X_test = test_data.drop('target', axis=1).astype(float)
            y_test = test_data['target'].astype(float)

            test_score = model.score(X_test, y_test)
            logger.info(f"Prediction model RÂ² score: {test_score:.4f}")

            # Get the most recent data for prediction
            latest_data = feature_df.iloc[-1:].astype(float)
            if latest_data.isnull().values.any():
                latest_data = latest_data.fillna(0)  # Handle any missing values in latest data

            # Make prediction for future movement
            predicted_return = float(model.predict(latest_data)[0])  # Ensure result is float

            # Get feature importances (coefficients)
            feature_importance = dict(zip(X_train.columns, model.coef_))

            # Sort features by absolute importance
            sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)
            top_features = dict(sorted_features[:5])  # Top 5 most important features

            # Calculate confidence metric based on historical accuracy
            from sklearn.metrics import mean_squared_error
            y_pred_test = model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred_test)
            rmse = np.sqrt(mse)

            # Normalize RMSE to get a confidence score between 0 and 1
            avg_abs_return = np.mean(np.abs(y_test))
            confidence = float(max(0, min(1, 1 - (rmse / (avg_abs_return * 2)))))  # Ensure result is float

            # Get current price for the symbol and ensure it's float
            current_price = float(close_prices[symbol].iloc[-1])

            # Calculate predicted price
            predicted_price = current_price * (1 + predicted_return)

            # Prepare results
            results = {
                "symbol": symbol,
                "current_price": current_price,
                "predicted_return": predicted_return,
                "predicted_price": predicted_price,
                "prediction_horizon": prediction_horizon,
                "prediction_direction": "up" if predicted_return > 0 else "down",
                "confidence": confidence,
                "model_r2": float(test_score),  # Ensure RÂ² is float
                "top_indicators": {k: float(v) for k, v in top_features.items()},  # Convert coefficients to float
                "prediction_timestamp": datetime.now()
            }

            # Save prediction to database
            prediction_record = {
                "target_symbol": symbol,
                "predicted_return": predicted_return,
                "prediction_horizon": prediction_horizon,
                "confidence": confidence,
                "model_type": "linear_regression",
                "prediction_time": datetime.now(),
                "timeframe": timeframe
                # Additional fields could be added if needed
            }

            # self.db_manager.save_prediction(prediction_record)

            logger.info(f"Successfully generated prediction for {symbol} with {prediction_horizon} periods horizon")
            return results

        except Exception as e:
            logger.error(f"Error in correlated movement prediction: {str(e)}")
            return {"error": str(e)}


    def analyze_market_regime_correlations(self, symbols: List[str],
                                           market_regimes: Dict[Tuple[datetime, datetime], str],
                                           timeframe: str = None) -> Dict[str, Dict[str, pd.DataFrame]]:

        # Use default timeframe from config if not specified
        timeframe = timeframe or self.config['default_timeframe']

        logger.info(
            f"Analyzing market regime correlations for {len(symbols)} symbols across {len(market_regimes)} regimes")

        # Dictionary to store results by regime
        regime_correlations = {}

        # Correlation types to calculate
        correlation_types = ['price', 'returns', 'volatility', 'volume']

        try:
            # Process each market regime
            for (regime_start, regime_end), regime_name in market_regimes.items():
                logger.info(f"Analyzing regime '{regime_name}' from {regime_start} to {regime_end}")

                # Skip regimes with invalid time ranges
                if regime_start >= regime_end:
                    logger.warning(f"Invalid time range for regime {regime_name}: {regime_start} to {regime_end}")
                    continue

                # Initialize regime entry in results dictionary if not exists
                if regime_name not in regime_correlations:
                    regime_correlations[regime_name] = {}

                # Calculate price correlation for this regime
                try:
                    price_corr = self.calculate_price_correlation(
                        symbols=symbols,
                        timeframe=timeframe,
                        start_time=regime_start,
                        end_time=regime_end
                    )
                    regime_correlations[regime_name]['price'] = price_corr
                    logger.debug(f"Calculated price correlation for regime {regime_name}")
                except Exception as e:
                    logger.warning(f"Error calculating price correlation for regime {regime_name}: {str(e)}")

                # Calculate returns correlation
                try:
                    returns_corr = self.calculate_returns_correlation(
                        symbols=symbols,
                        timeframe=timeframe,
                        start_time=regime_start,
                        end_time=regime_end
                    )
                    regime_correlations[regime_name]['returns'] = returns_corr
                    logger.debug(f"Calculated returns correlation for regime {regime_name}")
                except Exception as e:
                    logger.warning(f"Error calculating returns correlation for regime {regime_name}: {str(e)}")

                # Calculate volatility correlation
                try:
                    volatility_corr = self.calculate_volatility_correlation(
                        symbols=symbols,
                        timeframe=timeframe,
                        start_time=regime_start,
                        end_time=regime_end
                    )
                    regime_correlations[regime_name]['volatility'] = volatility_corr
                    logger.debug(f"Calculated volatility correlation for regime {regime_name}")
                except Exception as e:
                    logger.warning(f"Error calculating volatility correlation for regime {regime_name}: {str(e)}")

                # Calculate volume correlation
                try:
                    volume_corr = self.calculate_volume_correlation(
                        symbols=symbols,
                        timeframe=timeframe,
                        start_time=regime_start,
                        end_time=regime_end
                    )
                    regime_correlations[regime_name]['volume'] = volume_corr
                    logger.debug(f"Calculated volume correlation for regime {regime_name}")
                except Exception as e:
                    logger.warning(f"Error calculating volume correlation for regime {regime_name}: {str(e)}")

                # Calculate average correlations for this regime
                if 'returns' in regime_correlations[regime_name]:
                    returns_matrix = regime_correlations[regime_name]['returns']

                    # Get average correlation value (excluding self-correlations)
                    corr_values = []
                    for i in range(len(symbols)):
                        for j in range(i + 1, len(symbols)):
                            corr_values.append(returns_matrix.iloc[i, j])

                    avg_corr = sum(corr_values) / len(corr_values) if corr_values else 0
                    logger.info(f"Regime {regime_name} - Average correlation: {avg_corr:.4f}")

            # Save regime correlation analysis to database
            for regime_name, correlation_data in regime_correlations.items():
                for corr_type, corr_matrix in correlation_data.items():
                    # Create record
                    regime_start, regime_end = None, None
                    for (start, end), name in market_regimes.items():
                        if name == regime_name:
                            regime_start, regime_end = start, end
                            break

                    if regime_start and regime_end:
                        record = {
                            'regime_name': regime_name,
                            'correlation_type': corr_type,
                            'timeframe': timeframe,
                            'start_time': regime_start,
                            'end_time': regime_end,
                            'analysis_time': datetime.now(),
                            'symbols': symbols,
                            'matrix_json': corr_matrix.to_json()
                        }

                        # Assuming we have a method to save regime correlation data
                        if hasattr(self.db_manager, 'save_market_regime_correlation'):
                            self.db_manager.save_market_regime_correlation(record)
                            logger.debug(f"Saved {corr_type} correlation for regime {regime_name} to database")

            logger.info(f"Market regime correlation analysis complete for {len(regime_correlations)} regimes")
            return regime_correlations

        except Exception as e:
            logger.error(f"Error analyzing market regime correlations: {str(e)}")
            raise


def main():
    """
    Ð¢ÐµÑÑ‚Ð¾Ð²Ð¸Ð¹ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ñ–Ñ— Ð¼Ð¾Ð¶Ð»Ð¸Ð²Ð¾ÑÑ‚ÐµÐ¹ ÐºÐ»Ð°ÑÑƒ MarketCorrelation
    """
    print("ÐŸÐ¾Ñ‡Ð°Ñ‚Ð¾Ðº Ñ‚ÐµÑÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ MarketCorrelation...")

    # Ð†Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ð¾Ð±'Ñ”ÐºÑ‚Ð° Ð°Ð½Ð°Ð»Ñ–Ð·Ð°Ñ‚Ð¾Ñ€Ð° ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—
    mc = MarketCorrelation()

    # Ð¡Ð¿Ð¸ÑÐ¾Ðº ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚Ð½Ð¸Ñ… Ð¿Ð°Ñ€ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ
    symbols = ['BTC', 'ETH','SOL']
    print(f"ÐÐ½Ð°Ð»Ñ–Ð·ÑƒÑ”Ð¼Ð¾ Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ñ– ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¸: {', '.join(symbols)}")

    # Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ Ñ‡Ð°ÑÐ¾Ð²Ð¸Ñ… Ñ€Ð°Ð¼Ð¾Ðº Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ
    end_time = datetime.now()
    start_time = end_time - timedelta(days=30)  # Ð´Ð°Ð½Ñ– Ð·Ð° Ð¾ÑÑ‚Ð°Ð½Ð½Ñ– 30 Ð´Ð½Ñ–Ð²
    print(f"ÐŸÐµÑ€Ñ–Ð¾Ð´ Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ: Ð· {start_time.strftime('%Y-%m-%d')} Ð¿Ð¾ {end_time.strftime('%Y-%m-%d')}")

    # Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ñƒ
    timeframe = '1h'  # 1-Ð³Ð¾Ð´Ð¸Ð½Ð½Ð¸Ð¹ Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼

    try:
        # 1. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ñ†Ñ–Ð½
        print("\n1. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ñ†Ñ–Ð½...")
        price_corr = mc.calculate_price_correlation(
            symbols=symbols,
            timeframe=timeframe,
            start_time=start_time,
            end_time=end_time
        )
        print("ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ñ†Ñ–Ð½:")
        print(price_corr.round(2))

        # 2. Ð—Ð½Ð°Ñ…Ð¾Ð´Ð¶ÐµÐ½Ð½Ñ Ð²Ð¸ÑÐ¾ÐºÐ¾ ÐºÐ¾Ñ€ÐµÐ»ÑŒÐ¾Ð²Ð°Ð½Ð¸Ñ… Ð¿Ð°Ñ€ Ð°ÐºÑ‚Ð¸Ð²Ñ–Ð²
        print("\n2. Ð’Ð¸ÑÐ¾ÐºÐ¾ ÐºÐ¾Ñ€ÐµÐ»ÑŒÐ¾Ð²Ð°Ð½Ñ– Ð¿Ð°Ñ€Ð¸:")
        correlated_pairs = mc.get_correlated_pairs(price_corr, threshold=0.7)
        for symbol1, symbol2, corr in correlated_pairs:
            print(f"{symbol1} Ñ– {symbol2}: {corr:.4f}")

        # 3. Ð—Ð½Ð°Ñ…Ð¾Ð´Ð¶ÐµÐ½Ð½Ñ Ð°Ð½Ñ‚Ð¸-ÐºÐ¾Ñ€ÐµÐ»ÑŒÐ¾Ð²Ð°Ð½Ð¸Ñ… Ð¿Ð°Ñ€ Ð°ÐºÑ‚Ð¸Ð²Ñ–Ð²
        print("\n3. ÐÐ½Ñ‚Ð¸-ÐºÐ¾Ñ€ÐµÐ»ÑŒÐ¾Ð²Ð°Ð½Ñ– Ð¿Ð°Ñ€Ð¸:")
        anticorrelated_pairs = mc.get_anticorrelated_pairs(price_corr, threshold=-0.3)
        for symbol1, symbol2, corr in anticorrelated_pairs:
            print(f"{symbol1} Ñ– {symbol2}: {corr:.4f}")

        # 4. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–
        print("\n4. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–...")
        returns_corr = mc.calculate_returns_correlation(
            symbols=symbols,
            timeframe=timeframe,
            start_time=start_time,
            end_time=end_time
        )
        print("ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–:")
        print(returns_corr.round(2))

        # 5. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–
        print("\n5. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–...")
        volatility_corr = mc.calculate_volatility_correlation(
            symbols=symbols,
            timeframe=timeframe,
            start_time=start_time,
            end_time=end_time
        )
        print("ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–:")
        print(volatility_corr.round(2))

        # 6. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð¾Ð±'Ñ”Ð¼Ñƒ Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ–
        print("\n6. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð¾Ð±'Ñ”Ð¼Ñƒ Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ–...")
        volume_corr = mc.calculate_volume_correlation(
            symbols=symbols,
            timeframe=timeframe,
            start_time=start_time,
            end_time=end_time
        )
        print("ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð¾Ð±'Ñ”Ð¼Ñƒ Ñ‚Ð¾Ñ€Ð³Ñ–Ð²Ð»Ñ–:")
        print(volume_corr.round(2))

        # 7. Ð’Ð¸Ð±Ñ–Ñ€ Ð¾Ð´Ð½Ñ–Ñ”Ñ— Ð¿Ð°Ñ€Ð¸ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ Ð´Ð¸Ð½Ð°Ð¼Ñ–Ñ‡Ð½Ð¾Ñ— ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—
        if correlated_pairs:
            symbol1, symbol2, _ = correlated_pairs[0]
            print(f"\n7. ÐÐ½Ð°Ð»Ñ–Ð· Ð´Ð¸Ð½Ð°Ð¼Ñ–Ñ‡Ð½Ð¾Ñ— ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð¼Ñ–Ð¶ {symbol1} Ñ– {symbol2}...")

            # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð·Ð¼Ñ–Ð½Ð½Ð¾Ñ— ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð· Ñ‡Ð°ÑÐ¾Ð¼
            rolling_corr = mc.calculate_rolling_correlation(
                symbol1=symbol1,
                symbol2=symbol2,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                window=24  # Ð²Ñ–ÐºÐ½Ð¾ Ñƒ 24 Ð³Ð¾Ð´Ð¸Ð½Ð¸
            )

            print(f"ÐŸÐ¾Ñ‚Ð¾Ñ‡Ð½Ð° ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ: {rolling_corr.iloc[-1]:.4f}")

            # Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð½Ñ Ð·Ð»Ð°Ð¼Ñ–Ð² Ñƒ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—
            print("\n8. Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð½Ñ Ð·Ð»Ð°Ð¼Ñ–Ð² ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—...")
            breakdown_points = mc.detect_correlation_breakdowns(
                symbol1=symbol1,
                symbol2=symbol2,
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time,
                threshold=0.2  # ÑÑƒÑ‚Ñ‚Ñ”Ð²Ð° Ð·Ð¼Ñ–Ð½Ð° ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—
            )

            print(f"Ð—Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {len(breakdown_points)} Ñ‚Ð¾Ñ‡Ð¾Ðº Ð·Ð»Ð°Ð¼Ñƒ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ—")
            if breakdown_points:
                for point in breakdown_points:
                    print(f"Ð—Ð»Ð°Ð¼ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ— Ð½Ð° {point}")

        # 9. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð±ÐµÑ‚Ð¸ Ð²Ñ–Ð´Ð½Ð¾ÑÐ½Ð¾ BTC
        print("\n9. Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð±ÐµÑ‚Ð¸ Ð²Ñ–Ð´Ð½Ð¾ÑÐ½Ð¾ BTC...")
        for symbol in symbols:
            if symbol == 'BTC':
                continue

            beta = mc.calculate_market_beta(
                symbol=symbol,
                market_symbol='BTC',
                timeframe=timeframe,
                start_time=start_time,
                end_time=end_time
            )

            if isinstance(beta, float):
                print(f"Ð‘ÐµÑ‚Ð° Ð´Ð»Ñ {symbol} Ð²Ñ–Ð´Ð½Ð¾ÑÐ½Ð¾ BTC: {beta:.4f}")

        # 10. ÐŸÐ¾ÑˆÑƒÐº Ð²ÐµÐ´ÑƒÑ‡Ð¸Ñ… Ñ–Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ–Ð²
        print("\n10. ÐŸÐ¾ÑˆÑƒÐº Ð²ÐµÐ´ÑƒÑ‡Ð¸Ñ… Ñ–Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ–Ð² Ð´Ð»Ñ BTCUSDT...")
        other_symbols = [s for s in symbols if s != 'BTC']
        leading_indicators = mc.find_leading_indicators(
            target_symbol='BTC',
            candidate_symbols=other_symbols,
            lag_periods=[1, 2, 3, 6, 12, 24],  # Ð»Ð°Ð³Ð¸ Ñƒ Ð³Ð¾Ð´Ð¸Ð½Ð°Ñ…
            timeframe=timeframe,
            start_time=start_time,
            end_time=end_time
        )

        print("Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ Ð¿Ð¾ÑˆÑƒÐºÑƒ Ð²ÐµÐ´ÑƒÑ‡Ð¸Ñ… Ñ–Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ–Ð²:")
        for symbol, lags in leading_indicators.items():
            best_lag = max(lags.items(), key=lambda x: abs(x[1]))
            print(f"{symbol} Ð½Ð° Ð»Ð°Ð³Ñƒ {best_lag[0]} Ð³Ð¾Ð´Ð¸Ð½: ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ {best_lag[1]:.4f}")

        # 11. ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð·ÑƒÐ²Ð°Ð½Ð½Ñ Ñ€ÑƒÑ…Ñƒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ– ÐºÐ¾Ñ€ÐµÐ»ÑŒÐ¾Ð²Ð°Ð½Ð¸Ñ… Ð°ÐºÑ‚Ð¸Ð²Ñ–Ð²
        print("\n11. ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð·ÑƒÐ²Ð°Ð½Ð½Ñ Ñ€ÑƒÑ…Ñƒ BTC Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ– ÐºÐ¾Ñ€ÐµÐ»ÑŒÐ¾Ð²Ð°Ð½Ð¸Ñ… Ð°ÐºÑ‚Ð¸Ð²Ñ–Ð²...")
        prediction = mc.correlated_movement_prediction(
            symbol='BTC',
            correlated_symbols=other_symbols,
            prediction_horizon=24,  # Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 24 Ð³Ð¾Ð´Ð¸Ð½Ð¸
            timeframe=timeframe
        )

        print("Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·ÑƒÐ²Ð°Ð½Ð½Ñ:")
        for key, value in prediction.items():
            print(f"{key}: {value}")

        # 12. ÐÐ½Ð°Ð»Ñ–Ð· ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ð¹ Ð·Ð° Ñ€Ñ–Ð·Ð½Ð¸Ñ… Ñ€Ð¸Ð½ÐºÐ¾Ð²Ð¸Ñ… Ñ€ÐµÐ¶Ð¸Ð¼Ñ–Ð²
        print("\n12. ÐÐ½Ð°Ð»Ñ–Ð· ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ð¹ Ð·Ð° Ñ€Ñ–Ð·Ð½Ð¸Ñ… Ñ€Ð¸Ð½ÐºÐ¾Ð²Ð¸Ñ… Ñ€ÐµÐ¶Ð¸Ð¼Ñ–Ð²...")
        # Ð’Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ñ€Ð¸Ð½ÐºÐ¾Ð²Ð¸Ñ… Ñ€ÐµÐ¶Ð¸Ð¼Ñ–Ð² (Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´)
        regime_start = start_time
        regime_mid = start_time + (end_time - start_time) / 2

        market_regimes = {
            (regime_start, regime_mid): "ÐŸÐµÑ€ÑˆÐ° Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð¿ÐµÑ€Ñ–Ð¾Ð´Ñƒ",
            (regime_mid, end_time): "Ð”Ñ€ÑƒÐ³Ð° Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð¿ÐµÑ€Ñ–Ð¾Ð´Ñƒ"
        }

        regime_correlations = mc.analyze_market_regime_correlations(
            symbols=symbols,
            market_regimes=market_regimes,
            timeframe=timeframe
        )

        print("Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ Ñ€Ð¸Ð½ÐºÐ¾Ð²Ð¸Ñ… Ñ€ÐµÐ¶Ð¸Ð¼Ñ–Ð²:")
        for regime_name, corr_data in regime_correlations.items():
            if 'returns' in corr_data:
                returns_matrix = corr_data['returns']
                avg_corr = returns_matrix.values[np.triu_indices_from(returns_matrix.values, k=1)].mean()
                print(f"Ð ÐµÐ¶Ð¸Ð¼ '{regime_name}' - ÑÐµÑ€ÐµÐ´Ð½Ñ ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ñ Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ñ–: {avg_corr:.4f}")

        print("\nÐ¢ÐµÑÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ MarketCorrelation Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ ÑƒÑÐ¿Ñ–ÑˆÐ½Ð¾!")

    except Exception as e:
        print(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ñ‚ÐµÑÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ–: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

