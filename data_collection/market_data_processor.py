import os
import traceback
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Tuple, Union, Optional
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import hashlib
import json
from functools import lru_cache
import pytz
from utils.config import db_connection
from data.db import DatabaseManager


class MarketDataProcessor:

    def __init__(self,  log_level=logging.INFO):
        self.log_level = log_level
        self.db_connection = db_connection
        self.db_manager = DatabaseManager()
        self.supported_symbols = self.db_manager.supported_symbols
        logging.basicConfig(level=self.log_level)
        self.logger = logging.getLogger(__name__)
        self.logger.info("–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—É...")
        self.ready = True

    def save_klines_to_db(self, df: pd.DataFrame, symbol: str, interval: str):
        if df.empty:
            self.logger.warning("–°–ø—Ä–æ–±–∞ –∑–±–µ—Ä–µ–≥—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ —Å–≤—ñ—á–∫–∏")
            return

        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, (np.generic, np.bool_)):
                return obj.item()
            else:
                return obj

        for _, row in df.iterrows():
            try:
                kline_data = {
                    'interval': interval,
                    'open_time': row.name,
                    'open': row['open'],
                    'high': row['high'],
                    'low': row['low'],
                    'close': row['close'],
                    'volume': row.get('volume', 0),
                    'close_time': row.get('close_time', row.name),
                    'quote_asset_volume': row.get('quote_asset_volume', 0),
                    'number_of_trades': row.get('number_of_trades', 0),
                    'taker_buy_base_volume': row.get('taker_buy_base_volume', 0),
                    'taker_buy_quote_volume': row.get('taker_buy_quote_volume', 0),
                    'is_closed': bool(row.get('is_closed', True)),
                }

                kline_data = convert_numpy_types(kline_data)

                self.db_manager.insert_kline(symbol, kline_data)

            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Å–≤—ñ—á–∫–∏ –¥–ª—è {symbol}: {e}")

    def save_processed_klines_to_db(self, df: pd.DataFrame, symbol: str, interval: str):
        if df.empty:
            self.logger.warning("–°–ø—Ä–æ–±–∞ –∑–±–µ—Ä–µ–≥—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ –æ–±—Ä–æ–±–ª–µ–Ω—ñ —Å–≤—ñ—á–∫–∏")
            return

        for _, row in df.iterrows():
            try:
                processed_data = {
                    'interval': interval,
                    'open_time': row.name,
                    'open': row['open'],
                    'high': row['high'],
                    'low': row['low'],
                    'close': row['close'],
                    'volume': row['volume'],
                    'price_zscore': row.get('price_zscore'),
                    'volume_zscore': row.get('volume_zscore'),
                    'volatility': row.get('volatility'),
                    'trend': row.get('trend'),
                    'hour': row.get('hour'),
                    'day_of_week': row.get('weekday'),
                    'is_weekend': bool(row.get('is_weekend')),  # –æ—á—ñ–∫—É—î—Ç—å—Å—è bool
                    'session': row.get('session', 'unknown'),
                    'is_anomaly': row.get('is_anomaly', False),
                    'has_missing': row.get('has_missing', False)
                }
                self.db_manager.insert_kline_processed(symbol, processed_data)
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –æ–±—Ä–æ–±–ª–µ–Ω–æ—ó —Å–≤—ñ—á–∫–∏: {e}")

    def save_volume_profile_to_db(self, df: pd.DataFrame, symbol: str, interval: str):
        if df.empty:
            self.logger.warning("–°–ø—Ä–æ–±–∞ –∑–±–µ—Ä–µ–≥—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–π –ø—Ä–æ—Ñ—ñ–ª—å –æ–±'—î–º—É")
            return

        for _, row in df.iterrows():
            try:
                profile_data = {
                    'interval': interval,
                    'time_bucket': row.get('period') or row.name,
                    'price_bin_start': row.get('bin_lower'),
                    'price_bin_end': row.get('bin_upper'),
                    'volume': row['volume']
                }
                self.db_manager.insert_volume_profile(symbol, profile_data)
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –ø—Ä–æ—Ñ—ñ–ª—é –æ–±'—î–º—É: {e}")

    def _load_from_database(self, symbol: str, interval: str,
                            start_date: Optional[datetime] = None,
                            end_date: Optional[datetime] = None,
                            data_type: str = 'candles') -> pd.DataFrame:

        self.logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {data_type} –¥–∞–Ω–∏—Ö –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –¥–ª—è {symbol} {interval}")

        try:
            data = None
            if data_type == 'candles':
                data = self.db_manager.get_klines(
                    symbol=symbol,
                    interval=interval,
                    start_time=start_date,
                    end_time=end_date
                )
            elif data_type == 'orderbook':
                data = self.db_manager.get_orderbook(
                    symbol=symbol,
                    start_time=start_date,
                    end_time=end_date
                )
            else:
                raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ç–∏–ø –¥–∞–Ω–∏—Ö: {data_type}")

            if data is None:
                self.logger.warning("–ë–∞–∑–∞ –¥–∞–Ω–∏—Ö –ø–æ–≤–µ—Ä–Ω—É–ª–∞ None")
                return pd.DataFrame()

            if not isinstance(data, pd.DataFrame):
                data = pd.DataFrame(data)

            if 'timestamp' in data.columns:
                data['timestamp'] = pd.to_datetime(data['timestamp'])
                data.set_index('timestamp', inplace=True)

            return data

        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö: {str(e)}")
            raise

    def load_data(self, data_source: str, symbol: str, interval: str,
                  start_date: Optional[Union[str, datetime]] = None,
                  end_date: Optional[Union[str, datetime]] = None,
                  file_path: Optional[str] = None,
                  data_type: str = 'candles') -> pd.DataFrame:

        start_date_dt = pd.to_datetime(start_date) if start_date else None
        end_date_dt = pd.to_datetime(end_date) if end_date else None

        self.logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ {data_source}: {symbol}, {interval}, {data_type}")

        try:
            if data_source == 'database':
                data = self._load_from_database(
                    symbol,
                    interval,
                    start_date_dt,
                    end_date_dt,
                    data_type
                )
            elif data_source == 'csv':
                if not file_path:
                    raise ValueError("–î–ª—è –¥–∂–µ—Ä–µ–ª–∞ 'csv' –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –≤–∫–∞–∑–∞—Ç–∏ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É (file_path)")

                self.logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ CSV —Ñ–∞–π–ª—É: {file_path}")
                data = pd.read_csv(file_path)

                if 'timestamp' in data.columns or 'date' in data.columns or 'time' in data.columns:
                    time_col = next((col for col in ['timestamp', 'date', 'time'] if col in data.columns), None)
                    if time_col:
                        data[time_col] = pd.to_datetime(data[time_col])
                        data.set_index(time_col, inplace=True)
                    else:
                        self.logger.warning("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É –≤ CSV —Ñ–∞–π–ª—ñ")

                if start_date_dt and isinstance(data.index, pd.DatetimeIndex):
                    data = data[data.index >= start_date_dt]
                if end_date_dt and isinstance(data.index, pd.DatetimeIndex):
                    data = data[data.index <= end_date_dt]

            else:
                raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–µ –¥–∂–µ—Ä–µ–ª–æ –¥–∞–Ω–∏—Ö: {data_source}")

            if data is None or data.empty:
                self.logger.warning(f"–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –≤—ñ–¥ {data_source}")
                return pd.DataFrame()

            return data

        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –¥–∞–Ω–∏—Ö: {str(e)}")
            raise

    def clean_data(self, data: pd.DataFrame, remove_outliers: bool = True,
                   fill_missing: bool = True) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è")
            return data

        self.logger.info(f"–ü–æ—á–∞—Ç–æ–∫ –æ—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {data.shape[0]} —Ä—è–¥–∫—ñ–≤, {data.shape[1]} —Å—Ç–æ–≤–ø—Ü—ñ–≤")
        result = data.copy()

        if not isinstance(result.index, pd.DatetimeIndex):
            try:
                time_cols = [col for col in result.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    self.logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ {time_col} –≤ —ñ–Ω–¥–µ–∫—Å —á–∞—Å—É")
                    result[time_col] = pd.to_datetime(result[time_col])
                    result.set_index(time_col, inplace=True)
                else:
                    self.logger.warning("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–∫—É –∑ —á–∞—Å–æ–º, —ñ–Ω–¥–µ–∫—Å –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –Ω–µ–∑–º—ñ–Ω–Ω–∏–º")
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞–Ω–Ω—ñ —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")

        if result.index.duplicated().any():
            dup_count = result.index.duplicated().sum()
            self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {dup_count} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —ñ–Ω–¥–µ–∫—Å—É, –≤–∏–¥–∞–ª–µ–Ω–Ω—è...")
            result = result[~result.index.duplicated(keep='first')]

        result = result.sort_index()

        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in result.columns:
                result[col] = pd.to_numeric(result[col], errors='coerce')

        if remove_outliers:
            self.logger.info("–í–∏–¥–∞–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å...")
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
            for col in price_cols:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –∞–±–æ —Å–µ—Ä—ñ—é
                if result[col].empty or result[col].isna().all():
                    continue

                Q1 = result[col].quantile(0.25)
                Q3 = result[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 3 * IQR
                upper_bound = Q3 + 3 * IQR

                outliers = (result[col] < lower_bound) | (result[col] > upper_bound)
                if outliers.any():
                    outlier_count = outliers.sum()
                    self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {outlier_count} –∞–Ω–æ–º–∞–ª—ñ–π –≤ –∫–æ–ª–æ–Ω—Ü—ñ {col}")
                    result.loc[outliers, col] = np.nan

        if fill_missing and result.isna().any().any():
            self.logger.info("–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å...")
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
            if price_cols and isinstance(result.index, pd.DatetimeIndex):
                result[price_cols] = result[price_cols].interpolate(method='time')

            if 'volume' in result.columns and result['volume'].isna().any():
                result['volume'] = result['volume'].fillna(0)

            numeric_cols = result.select_dtypes(include=[np.number]).columns
            other_numeric = [col for col in numeric_cols if col not in price_cols + ['volume']]
            if other_numeric:
                if isinstance(result.index, pd.DatetimeIndex):
                    result[other_numeric] = result[other_numeric].interpolate(method='time')
                else:
                    result[other_numeric] = result[other_numeric].interpolate(method='linear')

            result = result.ffill().bfill()

        price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
        if len(price_cols) == 4:
            invalid_hl = result['high'] < result['low']
            if invalid_hl.any():
                invalid_count = invalid_hl.sum()
                self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {invalid_count} —Ä—è–¥–∫—ñ–≤, –¥–µ high < low")

                temp = result.loc[invalid_hl, 'high'].copy()
                result.loc[invalid_hl, 'high'] = result.loc[invalid_hl, 'low']
                result.loc[invalid_hl, 'low'] = temp

        self.logger.info(f"–û—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {result.shape[0]} —Ä—è–¥–∫—ñ–≤, {result.shape[1]} —Å—Ç–æ–≤–ø—Ü—ñ–≤")
        return result

    def resample_data(self, data: pd.DataFrame, target_interval: str) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥—É")
            return data

        if not isinstance(data.index, pd.DatetimeIndex):
            raise ValueError("–î–∞–Ω—ñ –ø–æ–≤–∏–Ω–Ω—ñ –º–∞—Ç–∏ DatetimeIndex –¥–ª—è —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥—É")

        required_cols = ['open', 'high', 'low', 'close']
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            self.logger.warning(f"–í—ñ–¥—Å—É—Ç–Ω—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
            return data

        pandas_interval = self._convert_interval_to_pandas_format(target_interval)
        self.logger.info(f"–†–µ—Å–µ–º–ø–ª—ñ–Ω–≥ –¥–∞–Ω–∏—Ö –¥–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É: {target_interval} (pandas —Ñ–æ—Ä–º–∞—Ç: {pandas_interval})")

        if len(data) > 1:
            current_interval = pd.Timedelta(data.index[1] - data.index[0])
            estimated_target_interval = self._parse_interval(target_interval)

            if estimated_target_interval < current_interval:
                self.logger.warning(f"–¶—ñ–ª—å–æ–≤–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª ({target_interval}) –º–µ–Ω—à–∏–π –∑–∞ –ø–æ—Ç–æ—á–Ω–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª –¥–∞–Ω–∏—Ö. "
                                    f"–î–∞—É–Ω—Å–µ–º–ø–ª—ñ–Ω–≥ –Ω–µ–º–æ–∂–ª–∏–≤–∏–π –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.")
                return data

        agg_dict = {
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last'
        }

        if 'volume' in data.columns:
            agg_dict['volume'] = 'sum'

        numeric_cols = data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col not in agg_dict:
                if any(x in col.lower() for x in ['count', 'number', 'trades']):
                    agg_dict[col] = 'sum'
                else:
                    agg_dict[col] = 'mean'

        try:
            resampled = data.resample(pandas_interval).agg(agg_dict)

            if resampled.isna().any().any():
                self.logger.info("–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –ø—ñ—Å–ª—è —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥—É...")
                price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in resampled.columns]
                resampled[price_cols] = resampled[price_cols].fillna(method='ffill')

                if 'volume' in resampled.columns:
                    resampled['volume'] = resampled['volume'].fillna(0)

            self.logger.info(f"–†–µ—Å–µ–º–ø–ª—ñ–Ω–≥ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {resampled.shape[0]} —Ä—è–¥–∫—ñ–≤")
            return resampled

        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥—É –¥–∞–Ω–∏—Ö: {str(e)}")
            raise

    def _convert_interval_to_pandas_format(self, interval: str) -> str:

        interval_map = {
            's': 'S',
            'm': 'T',
            'h': 'H',
            'd': 'D',
            'w': 'W',
            'M': 'M',
        }

        if not interval or not isinstance(interval, str):
            raise ValueError(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É: {interval}")

        import re
        match = re.match(r'(\d+)([smhdwM])', interval)
        if not match:
            raise ValueError(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É: {interval}")

        number, unit = match.groups()

        if unit in interval_map:
            return f"{number}{interval_map[unit]}"
        else:
            raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∞ –æ–¥–∏–Ω–∏—Ü—è —á–∞—Å—É: {unit}")

    def _parse_interval(self, interval: str) -> pd.Timedelta:

        interval_map = {
            's': 'seconds',
            'm': 'minutes',
            'h': 'hours',
            'd': 'days',
            'w': 'weeks',
        }

        import re
        match = re.match(r'(\d+)([smhdwM])', interval)
        if not match:
            raise ValueError(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É: {interval}")

        number, unit = match.groups()

        if unit == 'M':
            return pd.Timedelta(days=int(number) * 30.44)

        return pd.Timedelta(**{interval_map[unit]: int(number)})

    def detect_outliers(self, data: pd.DataFrame, method: str = 'zscore',
                        threshold: float = 3) -> Tuple[pd.DataFrame, List]:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π")
            return pd.DataFrame(), []

        self.logger.info(f"–ü–æ—á–∞—Ç–æ–∫ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π –º–µ—Ç–æ–¥–æ–º {method} –∑ –ø–æ—Ä–æ–≥–æ–º {threshold}")

        numeric_cols = data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            self.logger.warning("–£ DataFrame –≤—ñ–¥—Å—É—Ç–Ω—ñ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∞–Ω–æ–º–∞–ª—ñ–π")
            return pd.DataFrame(), []

        outliers_df = pd.DataFrame(index=data.index)
        all_outlier_indices = set()

        if method == 'zscore':
            for col in numeric_cols:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è == 0
                std = data[col].std()
                if std == 0 or pd.isna(std):
                    self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –º–∞—î –Ω—É–ª—å–æ–≤–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –∞–±–æ NaN")
                    continue

                z_scores = np.abs((data[col] - data[col].mean()) / std)
                outliers = z_scores > threshold
                outliers_df[f'{col}_outlier'] = outliers

                if outliers.any():
                    self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {outliers.sum()} –∞–Ω–æ–º–∞–ª—ñ–π —É –∫–æ–ª–æ–Ω—Ü—ñ {col} (zscore)")
                    all_outlier_indices.update(data.index[outliers])

        elif method == 'iqr':
            for col in numeric_cols:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥–æ—Å—Ç–∞—Ç–Ω—é –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –∫–≤–∞—Ä—Ç–∏–ª—ñ–≤
                if len(data[col].dropna()) < 4:
                    self.logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö —É –∫–æ–ª–æ–Ω—Ü—ñ {col} –¥–ª—è IQR –º–µ—Ç–æ–¥—É")
                    continue

                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1

                if IQR == 0 or pd.isna(IQR):
                    self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –º–∞—î –Ω—É–ª—å–æ–≤–∏–π IQR –∞–±–æ NaN")
                    continue

                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR

                outliers = (data[col] < lower_bound) | (data[col] > upper_bound)
                outliers_df[f'{col}_outlier'] = outliers

                if outliers.any():
                    self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {outliers.sum()} –∞–Ω–æ–º–∞–ª—ñ–π —É –∫–æ–ª–æ–Ω—Ü—ñ {col} (IQR)")
                    all_outlier_indices.update(data.index[outliers])

        elif method == 'isolation_forest':
            try:
                from sklearn.ensemble import IsolationForest

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
                if len(data) < 10:
                    self.logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è Isolation Forest")
                    return pd.DataFrame(), []

                X = data[numeric_cols].fillna(data[numeric_cols].mean())

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å NaN –ø—ñ—Å–ª—è –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è
                if X.isna().any().any():
                    self.logger.warning("–ó–∞–ª–∏—à–∏–ª–∏—Å—å NaN –ø—ñ—Å–ª—è –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è. –í–æ–Ω–∏ –±—É–¥—É—Ç—å –∑–∞–º—ñ–Ω–µ–Ω—ñ –Ω–∞ 0")
                    X = X.fillna(0)

                model = IsolationForest(contamination=min(0.1, 1 / threshold), random_state=42)
                predictions = model.fit_predict(X)

                outliers = predictions == -1

                outliers_df['isolation_forest_outlier'] = outliers

                if outliers.any():
                    self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {outliers.sum()} –∞–Ω–æ–º–∞–ª—ñ–π –º–µ—Ç–æ–¥–æ–º Isolation Forest")
                    all_outlier_indices.update(data.index[outliers])

            except ImportError:
                self.logger.error("–î–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É 'isolation_forest' –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ scikit-learn")
                return pd.DataFrame(), []

        else:
            self.logger.error(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π –º–µ—Ç–æ–¥ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π: {method}")
            return pd.DataFrame(), []

        if not outliers_df.empty:
            outliers_df['is_outlier'] = outliers_df.any(axis=1)

        outlier_indices = list(all_outlier_indices)

        self.logger.info(f"–í–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ó–Ω–∞–π–¥–µ–Ω–æ {len(outlier_indices)} –∞–Ω–æ–º–∞–ª—ñ–π —É –≤—Å—ñ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö")
        return outliers_df, outlier_indices

    def handle_missing_values(self, data: pd.DataFrame, method: str = 'interpolate',
                              symbol: str = None, interval: str = None,
                              fetch_missing: bool = False) -> pd.DataFrame:
        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å")
            return data

        result = data.copy()
        missing_values = result.isna().sum()
        total_missing = missing_values.sum()

        if total_missing == 0:
            self.logger.info("–í—ñ–¥—Å—É—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return result

        self.logger.info(
            f"–ó–Ω–∞–π–¥–µ–Ω–æ {total_missing} –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å —É {len(missing_values[missing_values > 0])} –∫–æ–ª–æ–Ω–∫–∞—Ö")

        # üîÑ –ü—ñ–¥—Ç—è–≥—É–≤–∞–Ω–Ω—è –∑ Binance
        if isinstance(result.index, pd.DatetimeIndex) and fetch_missing:
            time_diff = result.index.to_series().diff()
            expected_diff = time_diff.dropna().median() if len(time_diff) > 5 else None

            if expected_diff and symbol and interval:
                missing_periods = self._detect_missing_periods(result, expected_diff)
                if missing_periods:
                    self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(missing_periods)} –ø—Ä–æ–≥–∞–ª–∏–Ω. –ü—ñ–¥—Ç—è–≥—É—î–º–æ –∑ Binance...")
                    filled = self._fetch_missing_data_from_binance(result, missing_periods, symbol, interval)
                    result = pd.concat([result, filled])
                    result = result[~result.index.duplicated(keep='last')].sort_index()

        filled_values = 0
        numeric_cols = result.select_dtypes(include=[np.number]).columns

        if method == 'interpolate':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É –ª—ñ–Ω—ñ–π–Ω–æ—ó —ñ–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—ó")
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
            other_cols = [col for col in numeric_cols if col not in price_cols]
            before_fill = result.count().sum()

            if price_cols:
                result[price_cols] = result[price_cols].interpolate(method='time')

            if other_cols:
                result[other_cols] = result[other_cols].interpolate().ffill().bfill()

            result = result.ffill().bfill()
            filled_values = result.count().sum() - before_fill

        elif method == 'ffill':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É forward/backward fill")
            before_fill = result.count().sum()
            result = result.ffill().bfill()
            filled_values = result.count().sum() - before_fill

        elif method == 'mean':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—ñ–º –∑–Ω–∞—á–µ–Ω–Ω—è–º")
            for col in numeric_cols:
                if missing_values[col] > 0:
                    if result[col].dropna().empty:
                        self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∑–Ω–∞—á–µ–Ω—å –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ")
                        continue
                    col_mean = result[col].mean()
                    if pd.notna(col_mean):
                        before = result[col].isna().sum()
                        result[col] = result[col].fillna(col_mean)
                        filled_values += before - result[col].isna().sum()

        elif method == 'median':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –º–µ–¥—ñ–∞–Ω–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º")
            for col in numeric_cols:
                if missing_values[col] > 0:
                    if result[col].dropna().empty:
                        self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∑–Ω–∞—á–µ–Ω—å –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ–¥—ñ–∞–Ω–∏")
                        continue
                    col_median = result[col].median()
                    if pd.notna(col_median):
                        before = result[col].isna().sum()
                        result[col] = result[col].fillna(col_median)
                        filled_values += before - result[col].isna().sum()

        else:
            self.logger.warning(f"–ú–µ—Ç–æ–¥ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è '{method}' –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è")

        remaining_missing = result.isna().sum().sum()
        if remaining_missing > 0:
            self.logger.warning(f"–ó–∞–ª–∏—à–∏–ª–æ—Å—è {remaining_missing} –Ω–µ–∑–∞–ø–æ–≤–Ω–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –ø—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏")

        self.logger.info(f"–ó–∞–ø–æ–≤–Ω–µ–Ω–æ {filled_values} –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –º–µ—Ç–æ–¥–æ–º '{method}'")
        return result

    def _detect_missing_periods(self, data: pd.DataFrame, expected_diff: pd.Timedelta) -> List[
        Tuple[datetime, datetime]]:

        if not isinstance(data.index, pd.DatetimeIndex) or data.empty:
            return []

        if expected_diff is None:
            self.logger.warning("expected_diff —î None, –Ω–µ–º–æ–∂–ª–∏–≤–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ñ –ø–µ—Ä—ñ–æ–¥–∏")
            return []

        sorted_index = data.index.sort_values()

        time_diff = sorted_index.to_series().diff()
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±—ñ–ª—å—à –±–µ–∑–ø–µ—á–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        large_gaps = time_diff[time_diff > expected_diff * 1.5]

        missing_periods = []
        for timestamp, gap in large_gaps.items():
            prev_timestamp = timestamp - gap

            # –ó–∞–ø–æ–±—ñ–≥–∞—î–º–æ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ–º—É –ø–µ—Ä–µ–ø–æ–≤–Ω–µ–Ω–Ω—é –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ missing_steps
            try:
                missing_steps = max(0, int(gap / expected_diff) - 1)
                if missing_steps > 0:
                    self.logger.info(
                        f"–í–∏—è–≤–ª–µ–Ω–æ –ø—Ä–æ–º—ñ–∂–æ–∫: {prev_timestamp} - {timestamp} ({missing_steps} –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤)")
                    missing_periods.append((prev_timestamp, timestamp))
            except (OverflowError, ZeroDivisionError) as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ missing_steps: {str(e)}")

        return missing_periods

    def _fetch_missing_data_from_binance(self, data: pd.DataFrame,
                                         missing_periods: List[Tuple[datetime, datetime]],
                                         symbol: str, interval: str) -> pd.DataFrame:
        if data is None or data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –¥–∞–Ω–∏–º–∏")
            return pd.DataFrame()

        if not symbol or not interval:
            self.logger.error("–ù–µ–≤–∞–ª—ñ–¥–Ω–∏–π symbol –∞–±–æ interval")
            return data

        try:
            from binance.client import Client
            api_key = self.config.get('BINANCE_API_KEY') or os.environ.get('BINANCE_API_KEY')
            api_secret = self.config.get('BINANCE_API_SECRET') or os.environ.get('BINANCE_API_SECRET')

            if not api_key or not api_secret:
                self.logger.error("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–ª—é—á—ñ API Binance")
                return data

            client = Client(api_key, api_secret)
            filled_data = data.copy()

            valid_intervals = ['1m', '1h', '4h', '1d']
            if interval not in valid_intervals:
                self.logger.error(f"–ù–µ–≤–∞–ª—ñ–¥–Ω–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª: {interval}")
                return data

            new_data_frames = []

            for start_time, end_time in missing_periods:
                try:
                    self.logger.info(f"üì• –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Binance: {symbol}, {interval}, {start_time} - {end_time}")
                    start_ms = int(start_time.timestamp() * 1000)
                    end_ms = int(end_time.timestamp() * 1000)

                    klines = client.get_historical_klines(
                        symbol=symbol,
                        interval=interval,
                        start_str=start_ms,
                        end_str=end_ms
                    )

                    if not klines:
                        self.logger.warning(f"‚ö†Ô∏è –ü–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ Binance: {start_time} - {end_time}")
                        continue

                    columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume',
                               'close_time', 'quote_asset_volume', 'number_of_trades',
                               'taker_buy_base_volume', 'taker_buy_quote_volume', 'ignore']
                    binance_df = pd.DataFrame(klines, columns=columns[:len(klines[0])])

                    binance_df['timestamp'] = pd.to_datetime(binance_df['timestamp'], unit='ms')
                    binance_df.set_index('timestamp', inplace=True)

                    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —á–∏—Å–ª–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
                    for col in ['open', 'high', 'low', 'close', 'volume']:
                        if col in binance_df.columns:
                            binance_df[col] = pd.to_numeric(binance_df[col], errors='coerce')

                    binance_df['is_closed'] = True

                    # –í–∏–±–∏—Ä–∞—î–º–æ –ª–∏—à–µ —Ç—ñ –∫–æ–ª–æ–Ω–∫–∏, —è–∫—ñ —î –≤ –æ–±–æ—Ö DataFrame
                    common_cols = data.columns.intersection(binance_df.columns)
                    if common_cols.empty:
                        self.logger.warning("‚ö†Ô∏è –ù–µ–º–∞—î —Å–ø—ñ–ª—å–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è")
                        continue

                    new_data = binance_df[common_cols]
                    new_data_frames.append(new_data)

                    self.logger.info(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(new_data)} –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")

                except Exception as e:
                    self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ Binance: {e}")

            if not new_data_frames:
                return data

            combined_new = pd.concat(new_data_frames)
            total_before = len(filled_data)
            filled_data = pd.concat([filled_data, combined_new])
            filled_data = filled_data[~filled_data.index.duplicated(keep='last')]
            filled_data = filled_data.sort_index()
            total_after = len(filled_data)

            added_count = total_after - total_before
            self.logger.info(f"üß© –ó–∞–≥–∞–ª–æ–º –¥–æ–¥–∞–Ω–æ {added_count} –Ω–æ–≤–∏—Ö —Ä—è–¥–∫—ñ–≤ –ø—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è")

            return filled_data

        except ImportError:
            self.logger.error("‚ùå –ú–æ–¥—É–ª—å binance –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")
            return data

    def normalize_data(self, data: pd.DataFrame, method: str = 'z-score',
                       columns: List[str] = None, exclude_columns: List[str] = None) -> Tuple[pd.DataFrame, Dict]:

        if data is None or data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            return data, None

        result = data.copy()

        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

        if columns is not None:
            normalize_cols = [col for col in columns if col in numeric_cols]
            if not normalize_cols:
                self.logger.warning("–ñ–æ–¥–Ω–∞ –∑ —É–∫–∞–∑–∞–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ —î —á–∏—Å–ª–æ–≤–æ—é")
                return result, None
        else:
            normalize_cols = numeric_cols

        if exclude_columns is not None:
            normalize_cols = [col for col in normalize_cols if col not in exclude_columns]

        if not normalize_cols:
            self.logger.warning("–ù–µ–º–∞—î —á–∏—Å–ª–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            return result, None

        self.logger.info(f"–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è {len(normalize_cols)} –∫–æ–ª–æ–Ω–æ–∫ –º–µ—Ç–æ–¥–æ–º {method}")

        X = result[normalize_cols].values

        scaler = None
        if method == 'z-score':
            scaler = StandardScaler()
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è StandardScaler (z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)")
        elif method == 'min-max':
            scaler = MinMaxScaler()
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è MinMaxScaler (min-max –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)")
        elif method == 'robust':
            scaler = RobustScaler()
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è RobustScaler (—Ä–æ–±–∞—Å—Ç–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)")
        else:
            self.logger.error(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó: {method}")
            return result, None

        try:
            # –û–±—Ä–æ–±–∫–∞ NaN –∑–Ω–∞—á–µ–Ω—å
            if np.isnan(X).any():
                self.logger.warning("–ó–Ω–∞–π–¥–µ–Ω–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è –≤ –¥–∞–Ω–∏—Ö. –ó–∞–º—ñ–Ω–∞ –Ω–∞ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫")
                for i, col in enumerate(normalize_cols):
                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ –≤—Å—è –∫–æ–ª–æ–Ω–∫–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ NaN
                    if np.all(np.isnan(X[:, i])):
                        self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –º—ñ—Å—Ç–∏—Ç—å –ª–∏—à–µ NaN –∑–Ω–∞—á–µ–Ω–Ω—è. –ó–∞–º—ñ–Ω–∞ –Ω–∞ 0.")
                        X[:, i] = 0
                    else:
                        col_mean = np.nanmean(X[:, i])
                        X[:, i] = np.nan_to_num(X[:, i], nan=col_mean)

            X_scaled = scaler.fit_transform(X)

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
            if not np.isfinite(X_scaled).all():
                self.logger.warning("–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó. –ó–∞–º—ñ–Ω–∞ –Ω–∞ 0.")
                X_scaled = np.nan_to_num(X_scaled, nan=0, posinf=0, neginf=0)

            for i, col in enumerate(normalize_cols):
                result[col] = X_scaled[:, i]

            self.logger.info(f"–£—Å–ø—ñ—à–Ω–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∫–æ–ª–æ–Ω–∫–∏: {normalize_cols}")

            scaler_meta = {
                'method': method,
                'columns': normalize_cols,
                'scaler': scaler
            }

            return result, scaler_meta

        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö: {str(e)}")
            return data, None

    def align_time_series(self, data_list: List[pd.DataFrame],
                          reference_index: int = 0) -> List[pd.DataFrame]:

        if not data_list:
            self.logger.warning("–ü–æ—Ä–æ–∂–Ω—ñ–π —Å–ø–∏—Å–æ–∫ DataFrame –¥–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è")
            return []

        if reference_index < 0 or reference_index >= len(data_list):
            self.logger.error(f"–ù–µ–≤—ñ—Ä–Ω–∏–π reference_index: {reference_index}. –ú–∞—î –±—É—Ç–∏ –≤—ñ–¥ 0 –¥–æ {len(data_list) - 1}")
            reference_index = 0

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–æ DatetimeIndex
        for i, df in enumerate(data_list):
            if df is None or df.empty:
                self.logger.warning(f"DataFrame {i} —î –ø–æ—Ä–æ–∂–Ω—ñ–º –∞–±–æ None")
                data_list[i] = pd.DataFrame()  # –ó–∞–º—ñ–Ω—é—î–º–æ –Ω–∞ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame
                continue

            if not isinstance(df.index, pd.DatetimeIndex):
                self.logger.warning(f"DataFrame {i} –Ω–µ –º–∞—î —á–∞—Å–æ–≤–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
                try:
                    time_cols = [col for col in df.columns if
                                 any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                    if time_cols:
                        df[time_cols[0]] = pd.to_datetime(df[time_cols[0]])
                        df.set_index(time_cols[0], inplace=True)
                        data_list[i] = df
                    else:
                        self.logger.error(f"–ù–µ–º–æ–∂–ª–∏–≤–æ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ DataFrame {i}: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É")
                        return []
                except Exception as e:
                    self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —ñ–Ω–¥–µ–∫—Å—É –¥–ª—è DataFrame {i}: {str(e)}")
                    return []

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –µ—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ DataFrame
        reference_df = data_list[reference_index]
        if reference_df is None or reference_df.empty:
            self.logger.error("–ï—Ç–∞–ª–æ–Ω–Ω–∏–π DataFrame —î –ø–æ—Ä–æ–∂–Ω—ñ–º")
            return data_list

        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —á–∞—Å—Ç–æ—Ç–∏ —á–∞—Å–æ–≤–æ–≥–æ —Ä—è–¥—É
        try:
            reference_freq = pd.infer_freq(reference_df.index)

            if not reference_freq:
                self.logger.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∞—Å—Ç–æ—Ç—É reference DataFrame. –°–ø—Ä–æ–±–∞ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –≤—Ä—É—á–Ω—É.")
                if len(reference_df.index) > 1:
                    time_diff = reference_df.index.to_series().diff().dropna()
                    if not time_diff.empty:
                        reference_freq = time_diff.median()
                        self.logger.info(f"–í–∏–∑–Ω–∞—á–µ–Ω–æ –º–µ–¥—ñ–∞–Ω–Ω–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª: {reference_freq}")
                    else:
                        self.logger.error("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —ñ–Ω—Ç–µ—Ä–≤–∞–ª –∑ —Ä—ñ–∑–Ω–∏—Ü—ñ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫")
                        return data_list
                else:
                    self.logger.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ç–æ—á–æ–∫ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —á–∞—Å—Ç–æ—Ç–∏ reference DataFrame")
                    return data_list
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—ñ —á–∞—Å—Ç–æ—Ç–∏ reference DataFrame: {str(e)}")
            return data_list

        aligned_data_list = [reference_df]

        for i, df in enumerate(data_list):
            if i == reference_index:
                continue

            if df is None or df.empty:
                self.logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame {i}")
                aligned_data_list.append(df)
                continue

            self.logger.info(f"–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è DataFrame {i} –∑ reference DataFrame")

            if df.index.equals(reference_df.index):
                aligned_data_list.append(df)
                continue

            try:
                start_time = max(df.index.min(), reference_df.index.min())
                end_time = min(df.index.max(), reference_df.index.max())

                reference_subset = reference_df.loc[(reference_df.index >= start_time) &
                                                    (reference_df.index <= end_time)]

                # –ë–µ–∑–ø–µ—á–Ω–∏–π —Å–ø–æ—Å—ñ–± reindex
                aligned_df = df.reindex(reference_subset.index, method=None)

                numeric_cols = aligned_df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    aligned_df[numeric_cols] = aligned_df[numeric_cols].interpolate(method='time')

                aligned_data_list.append(aligned_df)

                missing_values = aligned_df.isna().sum().sum()
                if missing_values > 0:
                    self.logger.warning(
                        f"–ü—ñ—Å–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è DataFrame {i} –∑–∞–ª–∏—à–∏–ª–æ—Å—è {missing_values} –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å")

            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—ñ DataFrame {i}: {str(e)}")
                self.logger.error(f"–î–µ—Ç–∞–ª—ñ –ø–æ–º–∏–ª–∫–∏: {traceback.format_exc()}")
                aligned_data_list.append(df)  # –î–æ–¥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ

        return aligned_data_list

    def validate_data_integrity(self, data: pd.DataFrame, price_jump_threshold: float = 0.2,
                                volume_anomaly_threshold: float = 5) -> Dict[str, List]:

        if data is None or data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ü—ñ–ª—ñ—Å–Ω–æ—Å—Ç—ñ")
            return {"empty_data": []}

        issues = {}

        expected_cols = ['open', 'high', 'low', 'close', 'volume']
        missing_cols = [col for col in expected_cols if col not in data.columns]
        if missing_cols:
            issues["missing_columns"] = missing_cols
            self.logger.warning(f"–í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")

        if not isinstance(data.index, pd.DatetimeIndex):
            issues["not_datetime_index"] = True
            self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex")
        else:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∞—Å–æ–≤–∏—Ö –ø—Ä–æ–º—ñ–∂–∫—ñ–≤
            if len(data.index) > 1:
                time_diff = data.index.to_series().diff().dropna()
                if not time_diff.empty:
                    median_diff = time_diff.median()

                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –≤–µ–ª–∏–∫—ñ –ø—Ä–æ–º—ñ–∂–∫–∏
                    if median_diff.total_seconds() > 0:  # –£–Ω–∏–∫–∞—î–º–æ –¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω—É–ª—å
                        large_gaps = time_diff[time_diff > 2 * median_diff]
                        if not large_gaps.empty:
                            gap_locations = large_gaps.index.tolist()
                            issues["time_gaps"] = gap_locations
                            self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(gap_locations)} –∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö –ø—Ä–æ–º—ñ–∂–∫—ñ–≤ —É —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–∫–∞—Ö")

                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫
                    duplicates = data.index.duplicated()
                    if duplicates.any():
                        dup_indices = data.index[duplicates].tolist()
                        issues["duplicate_timestamps"] = dup_indices
                        self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(dup_indices)} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ü—ñ–Ω–æ–≤–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π
        price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in data.columns]
        if len(price_cols) == 4:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ high < low
            invalid_hl = data['high'] < data['low']
            if invalid_hl.any():
                invalid_hl_indices = data.index[invalid_hl].tolist()
                issues["high_lower_than_low"] = invalid_hl_indices
                self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(invalid_hl_indices)} –∑–∞–ø–∏—Å—ñ–≤ –¥–µ high < low")

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤—ñ–¥'—î–º–Ω–∏—Ö —Ü—ñ–Ω
            for col in price_cols:
                negative_prices = data[col] < 0
                if negative_prices.any():
                    neg_price_indices = data.index[negative_prices].tolist()
                    issues[f"negative_{col}"] = neg_price_indices
                    self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(neg_price_indices)} –∑–∞–ø–∏—Å—ñ–≤ –∑ –≤—ñ–¥'—î–º–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ —É {col}")

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä—ñ–∑–∫–∏—Ö —Å—Ç—Ä–∏–±–∫—ñ–≤ —Ü—ñ–Ω
            for col in price_cols:
                pct_change = data[col].pct_change().abs()
                price_jumps = pct_change > price_jump_threshold
                if price_jumps.any():
                    jump_indices = data.index[price_jumps].tolist()
                    issues[f"price_jumps_{col}"] = jump_indices
                    self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(jump_indices)} —Ä—ñ–∑–∫–∏—Ö –∑–º—ñ–Ω —É –∫–æ–ª–æ–Ω—Ü—ñ {col}")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ–±'—î–º—É
        if 'volume' in data.columns:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤—ñ–¥'—î–º–Ω–æ–≥–æ –æ–±'—î–º—É
            negative_volume = data['volume'] < 0
            if negative_volume.any():
                neg_vol_indices = data.index[negative_volume].tolist()
                issues["negative_volume"] = neg_vol_indices
                self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(neg_vol_indices)} –∑–∞–ø–∏—Å—ñ–≤ –∑ –≤—ñ–¥'—î–º–Ω–∏–º –æ–±'—î–º–æ–º")

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω–æ–≥–æ –æ–±'—î–º—É
            try:
                volume_std = data['volume'].std()
                if volume_std > 0:  # –£–Ω–∏–∫–∞—î–º–æ –¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω—É–ª—å
                    volume_zscore = np.abs((data['volume'] - data['volume'].mean()) / volume_std)
                    volume_anomalies = volume_zscore > volume_anomaly_threshold
                    if volume_anomalies.any():
                        vol_anomaly_indices = data.index[volume_anomalies].tolist()
                        issues["volume_anomalies"] = vol_anomaly_indices
                        self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(vol_anomaly_indices)} –∑–∞–ø–∏—Å—ñ–≤ –∑ –∞–Ω–æ–º–∞–ª—å–Ω–∏–º –æ–±'—î–º–æ–º")
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ –∞–Ω–æ–º–∞–ª—ñ–π –æ–±'—î–º—É: {str(e)}")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ NaN –∑–Ω–∞—á–µ–Ω–Ω—è
        na_counts = data.isna().sum()
        cols_with_na = na_counts[na_counts > 0].index.tolist()
        if cols_with_na:
            issues["columns_with_na"] = {col: data.index[data[col].isna()].tolist() for col in cols_with_na}
            self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —É –∫–æ–ª–æ–Ω–∫–∞—Ö: {cols_with_na}")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        try:
            inf_counts = np.isinf(data.select_dtypes(include=[np.number])).sum()
            cols_with_inf = inf_counts[inf_counts > 0].index.tolist()
            if cols_with_inf:
                issues["columns_with_inf"] = {col: data.index[np.isinf(data[col])].tolist() for col in cols_with_inf}
                self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —É –∫–æ–ª–æ–Ω–∫–∞—Ö: {cols_with_inf}")
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å: {str(e)}")

        return issues

    def aggregate_volume_profile(self, data: pd.DataFrame, bins: int = 10,
                                 price_col: str = 'close', volume_col: str = 'volume',
                                 time_period: Optional[str] = None) -> pd.DataFrame:

        if data is None or data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –ø—Ä–æ—Ñ—ñ–ª—é –æ–±'—î–º—É")
            return pd.DataFrame()

        if price_col not in data.columns:
            self.logger.error(f"–ö–æ–ª–æ–Ω–∫–∞ {price_col} –≤—ñ–¥—Å—É—Ç–Ω—è —É DataFrame")
            return pd.DataFrame()

        if volume_col not in data.columns:
            self.logger.error(f"–ö–æ–ª–æ–Ω–∫–∞ {volume_col} –≤—ñ–¥—Å—É—Ç–Ω—è —É DataFrame")
            return pd.DataFrame()

        self.logger.info(f"–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é –æ–±'—î–º—É –∑ {bins} —Ü—ñ–Ω–æ–≤–∏–º–∏ —Ä—ñ–≤–Ω—è–º–∏")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —á–∞—Å–æ–≤–æ–≥–æ –ø—Ä–æ—Ñ—ñ–ª—é
        if time_period:
            if not isinstance(data.index, pd.DatetimeIndex):
                self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex. –ß–∞—Å–æ–≤–∏–π –ø—Ä–æ—Ñ—ñ–ª—å –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ —Å—Ç–≤–æ—Ä–µ–Ω–∏–π.")
                # –°—Ç–≤–æ—Ä—é—î–º–æ –ø—Ä–æ—Å—Ç–∏–π –ø—Ä–æ—Ñ—ñ–ª—å –æ–±'—î–º—É –∑–∞–º—ñ—Å—Ç—å —á–∞—Å–æ–≤–æ–≥–æ
                return self._create_volume_profile(data, bins, price_col, volume_col)

            self.logger.info(f"–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —á–∞—Å–æ–≤–æ–≥–æ –ø—Ä–æ—Ñ—ñ–ª—é –æ–±'—î–º—É –∑ –ø–µ—Ä—ñ–æ–¥–æ–º {time_period}")
            period_groups = data.groupby(pd.Grouper(freq=time_period))

            result_dfs = []

            for period, group in period_groups:
                if group.empty:
                    continue

                period_profile = self._create_volume_profile(group, bins, price_col, volume_col)
                if not period_profile.empty:
                    period_profile['period'] = period
                    result_dfs.append(period_profile)

            if result_dfs:
                return pd.concat(result_dfs)
            else:
                self.logger.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ —á–∞—Å–æ–≤–∏–π –ø—Ä–æ—Ñ—ñ–ª—å –æ–±'—î–º—É")
                return pd.DataFrame()
        else:
            return self._create_volume_profile(data, bins, price_col, volume_col)

    def _create_volume_profile(self, data: pd.DataFrame, bins: int,
                               price_col: str, volume_col: str) -> pd.DataFrame:

        price_min = data[price_col].min()
        price_max = data[price_col].max()

        if price_min == price_max:
            self.logger.warning("–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ü—ñ–Ω–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ. –ù–µ–º–æ–∂–ª–∏–≤–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ—Ñ—ñ–ª—å –æ–±'—î–º—É.")
            return pd.DataFrame()

        effective_bins = min(bins, int((price_max - price_min) * 100) + 1)
        if effective_bins < bins:
            self.logger.warning(f"–ó–º–µ–Ω—à–µ–Ω–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –±—ñ–Ω—ñ–≤ –∑ {bins} –¥–æ {effective_bins} —á–µ—Ä–µ–∑ –º–∞–ª–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω —Ü—ñ–Ω")
            bins = effective_bins

        if bins <= 1:
            self.logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –±—ñ–Ω—ñ–≤ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é –æ–±'—î–º—É")
            return pd.DataFrame()

        try:
            bin_edges = np.linspace(price_min, price_max, bins + 1)
            bin_width = (price_max - price_min) / bins

            bin_labels = list(range(bins))
            data = data.copy()  # –≥–∞—Ä–∞–Ω—Ç—ñ—è, —â–æ –Ω–µ –∑–º—ñ–Ω—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª

            # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ SettingWithCopyWarning
            data.loc[:, 'price_bin'] = pd.cut(
                data[price_col],
                bins=bin_edges,
                labels=bin_labels,
                include_lowest=True
            )

            # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ FutureWarning ‚Äî –¥–æ–¥–∞–Ω–æ observed=False
            volume_profile = data.groupby('price_bin', observed=False).agg({
                volume_col: 'sum',
                price_col: ['count', 'min', 'max']
            })

            if volume_profile.empty:
                self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π –ø—Ä–æ—Ñ—ñ–ª—å –æ–±'—î–º—É –ø—ñ—Å–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è")
                return pd.DataFrame()

            volume_profile.columns = [f'{col[0]}_{col[1]}' if col[1] else col[0] for col in volume_profile.columns]
            volume_profile = volume_profile.rename(columns={
                f'{volume_col}_sum': 'volume',
                f'{price_col}_count': 'count',
                f'{price_col}_min': 'price_min',
                f'{price_col}_max': 'price_max'
            })

            total_volume = volume_profile['volume'].sum()
            if total_volume > 0:
                volume_profile['volume_percent'] = (volume_profile['volume'] / total_volume * 100).round(2)
            else:
                volume_profile['volume_percent'] = 0

            volume_profile['price_mid'] = (volume_profile['price_min'] + volume_profile['price_max']) / 2

            volume_profile['bin_lower'] = [bin_edges[i] for i in volume_profile.index]
            volume_profile['bin_upper'] = [bin_edges[i + 1] for i in volume_profile.index]

            volume_profile = volume_profile.reset_index()
            volume_profile = volume_profile.sort_values('price_bin', ascending=False)

            if 'price_bin' in volume_profile.columns:
                volume_profile = volume_profile.drop('price_bin', axis=1)

            return volume_profile

        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –ø—Ä–æ—Ñ—ñ–ª—é –æ–±'—î–º—É: {str(e)}")
            return pd.DataFrame()

    def add_time_features(self, data: pd.DataFrame, cyclical: bool = True,
                          add_sessions: bool = False, tz: str = 'Europe/Kiev') -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")
            return data

        result = data.copy()

        if not isinstance(result.index, pd.DatetimeIndex):
            self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
            try:
                time_cols = [col for col in result.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    result[time_col] = pd.to_datetime(result[time_col])
                    result.set_index(time_col, inplace=True)
                else:
                    self.logger.error("–ù–µ–º–æ–∂–ª–∏–≤–æ –¥–æ–¥–∞—Ç–∏ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É")
                    return data
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")
                return data

        self.logger.info("–î–æ–¥–∞–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")

        if result.index.tz is None:
            self.logger.info(f"–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É {tz}")
            try:
                result.index = result.index.tz_localize(tz)
            except Exception as e:
                self.logger.warning(
                    f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É: {str(e)}. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É.")
        elif result.index.tz.zone != tz:
            self.logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É –∑ {result.index.tz.zone} –≤ {tz}")
            try:
                result.index = result.index.tz_convert(tz)
            except Exception as e:
                self.logger.warning(
                    f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É: {str(e)}. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ –ø–æ—Ç–æ—á–Ω–∏–º —á–∞—Å–æ–≤–∏–º –ø–æ—è—Å–æ–º.")

        result['hour'] = result.index.hour
        result['day'] = result.index.day
        result['weekday'] = result.index.weekday
        result['week'] = result.index.isocalendar().week
        result['month'] = result.index.month
        result['quarter'] = result.index.quarter
        result['year'] = result.index.year
        result['dayofyear'] = result.index.dayofyear

        result['is_weekend'] = result['weekday'].isin([5, 6]).astype(int)
        result['is_month_start'] = result.index.is_month_start.astype(int)
        result['is_month_end'] = result.index.is_month_end.astype(int)
        result['is_quarter_start'] = result.index.is_quarter_start.astype(int)
        result['is_quarter_end'] = result.index.is_quarter_end.astype(int)
        result['is_year_start'] = result.index.is_year_start.astype(int)
        result['is_year_end'] = result.index.is_year_end.astype(int)

        if cyclical:
            self.logger.info("–î–æ–¥–∞–≤–∞–Ω–Ω—è —Ü–∏–∫–ª—ñ—á–Ω–∏—Ö –æ–∑–Ω–∞–∫")

            result['hour_sin'] = np.sin(2 * np.pi * result['hour'] / 24)
            result['hour_cos'] = np.cos(2 * np.pi * result['hour'] / 24)

            days_in_month = result.index.days_in_month
            result['day_sin'] = np.sin(2 * np.pi * result['day'] / days_in_month)
            result['day_cos'] = np.cos(2 * np.pi * result['day'] / days_in_month)

            result['weekday_sin'] = np.sin(2 * np.pi * result['weekday'] / 7)
            result['weekday_cos'] = np.cos(2 * np.pi * result['weekday'] / 7)

            result['week_sin'] = np.sin(2 * np.pi * result['week'] / 52)
            result['week_cos'] = np.cos(2 * np.pi * result['week'] / 52)

            result['month_sin'] = np.sin(2 * np.pi * result['month'] / 12)
            result['month_cos'] = np.cos(2 * np.pi * result['month'] / 12)

            result['quarter_sin'] = np.sin(2 * np.pi * result['quarter'] / 4)
            result['quarter_cos'] = np.cos(2 * np.pi * result['quarter'] / 4)

        if add_sessions:
            self.logger.info("–î–æ–¥–∞–≤–∞–Ω–Ω—è —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ —Ç–æ—Ä–≥–æ–≤–∏—Ö —Å–µ—Å—ñ–π")

            result['asian_session'] = ((result['hour'] >= 0) & (result['hour'] < 9)).astype(int)

            result['european_session'] = ((result['hour'] >= 8) & (result['hour'] < 17)).astype(int)

            result['american_session'] = ((result['hour'] >= 13) & (result['hour'] < 22)).astype(int)

            result['asia_europe_overlap'] = ((result['hour'] >= 8) & (result['hour'] < 9)).astype(int)
            result['europe_america_overlap'] = ((result['hour'] >= 13) & (result['hour'] < 17)).astype(int)

            result['inactive_hours'] = ((result['hour'] >= 22) | (result['hour'] < 0)).astype(int)

        self.logger.info(f"–£—Å–ø—ñ—à–Ω–æ –¥–æ–¥–∞–Ω–æ {len(result.columns) - len(data.columns)} —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")
        return result


    def remove_duplicate_timestamps(self, data: pd.DataFrame) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤")
            return data

        if not isinstance(data.index, pd.DatetimeIndex):
            self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
            try:
                time_cols = [col for col in data.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    data[time_col] = pd.to_datetime(data[time_col])
                    data.set_index(time_col, inplace=True)
                else:
                    self.logger.error("–ù–µ–º–æ–∂–ª–∏–≤–æ –≤–∏—è–≤–∏—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É")
                    return data
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")
                return data

        duplicates = data.index.duplicated()
        duplicates_count = duplicates.sum()

        if duplicates_count == 0:
            self.logger.info("–î—É–±–ª—ñ–∫–∞—Ç–∏ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return data

        self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {duplicates_count} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫")

        result = data[~duplicates]

        self.logger.info(f"–í–∏–¥–∞–ª–µ–Ω–æ {duplicates_count} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {len(result)} –∑–∞–ø–∏—Å—ñ–≤.")
        return result

    def filter_by_time_range(self, data: pd.DataFrame,
                             start_time: Optional[Union[str, datetime]] = None,
                             end_time: Optional[Union[str, datetime]] = None) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó")
            return data

        if not isinstance(data.index, pd.DatetimeIndex):
            self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
            try:
                time_cols = [col for col in data.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    data[time_col] = pd.to_datetime(data[time_col])
                    data.set_index(time_col, inplace=True)
                else:
                    self.logger.error("–ù–µ–º–æ–∂–ª–∏–≤–æ —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ –∑–∞ —á–∞—Å–æ–º: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É")
                    return data
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")
                return data

        result = data.copy()
        initial_count = len(result)

        if start_time is not None:
            try:
                start_dt = pd.to_datetime(start_time)
                result = result[result.index >= start_dt]
                self.logger.info(f"–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –ø–æ—á–∞—Ç–∫–æ–≤–∏–º —á–∞—Å–æ–º: {start_dt}")
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó –ø–æ—á–∞—Ç–∫–æ–≤–æ–≥–æ —á–∞—Å—É: {str(e)}")

        if end_time is not None:
            try:
                end_dt = pd.to_datetime(end_time)
                result = result[result.index <= end_dt]
                self.logger.info(f"–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –∫—ñ–Ω—Ü–µ–≤–∏–º —á–∞—Å–æ–º: {end_dt}")
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó –∫—ñ–Ω—Ü–µ–≤–æ–≥–æ —á–∞—Å—É: {str(e)}")

        final_count = len(result)

        if start_time is not None and end_time is not None:
            start_dt = pd.to_datetime(start_time)
            end_dt = pd.to_datetime(end_time)
            if start_dt > end_dt:
                self.logger.warning(f"–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —á–∞—Å ({start_dt}) –ø—ñ–∑–Ω—ñ—à–µ –∫—ñ–Ω—Ü–µ–≤–æ–≥–æ —á–∞—Å—É ({end_dt})")

        self.logger.info(f"–í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {initial_count - final_count} –∑–∞–ø–∏—Å—ñ–≤. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {final_count} –∑–∞–ø–∏—Å—ñ–≤.")
        return result

    def save_processed_data(self, data: pd.DataFrame, filename: str, db_connection=None) -> str:

        if data.empty:
            self.logger.warning("–°–ø—Ä–æ–±–∞ –∑–±–µ—Ä–µ–≥—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame")
            return ""

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö, —è–∫—â–æ –Ω–∞–¥–∞–Ω–æ –∑'—î–¥–Ω–∞–Ω–Ω—è
        if db_connection:
            try:
                table_name = os.path.basename(filename).split('.')[0]
                data.to_sql(table_name, db_connection, if_exists='replace', index=True)
                self.logger.info(f"–î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö, —Ç–∞–±–ª–∏—Ü—è: {table_name}")
                return table_name
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö: {str(e)}")
                return ""

        # –ó–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è —Ñ–æ—Ä–º–∞—Ç—É CSV
        if '.' in filename and filename.split('.')[-1].lower() != 'csv':
            filename = f"{filename.split('.')[0]}.csv"
            self.logger.warning(f"–ó–º—ñ–Ω–µ–Ω–æ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª—É –Ω–∞ CSV: {filename}")

        if not filename.endswith('.csv'):
            filename = f"{filename}.csv"

        directory = os.path.dirname(filename)
        if directory and not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)
            self.logger.info(f"–°—Ç–≤–æ—Ä–µ–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é: {directory}")

        try:
            data.to_csv(filename)
            self.logger.info(f"–î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É CSV —Ñ–æ—Ä–º–∞—Ç—ñ: {filename}")
            return os.path.abspath(filename)
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –¥–∞–Ω–∏—Ö: {str(e)}")
            return ""

    def load_processed_data(self, filename: str) -> pd.DataFrame:

        if not os.path.exists(filename):
            self.logger.error(f"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {filename}")
            return pd.DataFrame()

        file_extension = filename.split('.')[-1].lower() if '.' in filename else ''

        try:
            if file_extension == 'csv':
                data = pd.read_csv(filename)
                self.logger.info(f"–î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ CSV —Ñ–∞–π–ª—É: {filename}")

                time_cols = [col for col in data.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    data[time_col] = pd.to_datetime(data[time_col])
                    data.set_index(time_col, inplace=True)
                    self.logger.info(f"–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —ñ–Ω–¥–µ–∫—Å –∑–∞ –∫–æ–ª–æ–Ω–∫–æ—é {time_col}")
            else:
                self.logger.error(f"–ü—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –ª–∏—à–µ —Ñ–æ—Ä–º–∞—Ç CSV, –æ—Ç—Ä–∏–º–∞–Ω–æ: {file_extension}")
                return pd.DataFrame()

            if not isinstance(data.index, pd.DatetimeIndex) and len(data) > 0:
                self.logger.warning("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –¥–∞–Ω—ñ –Ω–µ –º–∞—é—Ç—å DatetimeIndex. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
                try:
                    time_cols = [col for col in data.columns if
                                 any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                    if time_cols:
                        time_col = time_cols[0]
                        data[time_col] = pd.to_datetime(data[time_col])
                        data.set_index(time_col, inplace=True)
                        self.logger.info(f"–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —ñ–Ω–¥–µ–∫—Å –∑–∞ –∫–æ–ª–æ–Ω–∫–æ—é {time_col}")
                except Exception as e:
                    self.logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ DatetimeIndex: {str(e)}")

            return data

        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –¥–∞–Ω–∏—Ö: {str(e)}")
            return pd.DataFrame()

    def merge_datasets(self, datasets: List[pd.DataFrame],
                       merge_on: str = 'timestamp') -> pd.DataFrame:

        if not datasets:
            self.logger.warning("–ü–æ—Ä–æ–∂–Ω—ñ–π —Å–ø–∏—Å–æ–∫ –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è")
            return pd.DataFrame()

        if len(datasets) == 1:
            return datasets[0].copy()

        self.logger.info(f"–ü–æ—á–∞—Ç–æ–∫ –æ–±'—î–¥–Ω–∞–Ω–Ω—è {len(datasets)} –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö")

        all_have_merge_on = all(merge_on in df.columns or df.index.name == merge_on for df in datasets)

        if not all_have_merge_on:
            if merge_on == 'timestamp':
                self.logger.info("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ –≤—Å—ñ DataFrame –º–∞—é—Ç—å DatetimeIndex")
                all_have_datetime_index = all(isinstance(df.index, pd.DatetimeIndex) for df in datasets)

                if all_have_datetime_index:
                    for i in range(len(datasets)):
                        if datasets[i].index.name is None:
                            datasets[i].index.name = 'timestamp'

                    all_have_merge_on = True

            if not all_have_merge_on:
                self.logger.error(f"–ù–µ –≤—Å—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö –º—ñ—Å—Ç—è—Ç—å '{merge_on}' –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è")
                return pd.DataFrame()

        datasets_copy = []
        for i, df in enumerate(datasets):
            df_copy = df.copy()

            if merge_on in df_copy.columns:
                df_copy.set_index(merge_on, inplace=True)
                self.logger.info(f"DataFrame {i} –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–æ: –∫–æ–ª–æ–Ω–∫–∞ '{merge_on}' —Å—Ç–∞–ª–∞ —ñ–Ω–¥–µ–∫—Å–æ–º")
            elif df_copy.index.name != merge_on:
                df_copy.index.name = merge_on
                self.logger.info(f"DataFrame {i}: —ñ–Ω–¥–µ–∫—Å –ø–µ—Ä–µ–π–º–µ–Ω–æ–≤–∞–Ω–æ –Ω–∞ '{merge_on}'")

            datasets_copy.append(df_copy)

        result = datasets_copy[0]
        total_columns = len(result.columns)

        for i, df in enumerate(datasets_copy[1:], 2):
            rename_dict = {}
            for col in df.columns:
                if col in result.columns:
                    rename_dict[col] = f"{col}_{i}"

            if rename_dict:
                self.logger.info(f"–ü–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫ —É DataFrame {i}: {rename_dict}")
                df = df.rename(columns=rename_dict)

            result = result.join(df, how='outer')
            total_columns += len(df.columns)

        self.logger.info(f"–û–±'—î–¥–Ω–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(result)} —Ä—è–¥–∫—ñ–≤, {len(result.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        self.logger.info(f"–ó {total_columns} –≤—Ö—ñ–¥–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫, {total_columns - len(result.columns)} –±—É–ª–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∞–º–∏")

        return result

    def preprocess_pipeline(self, data: pd.DataFrame,
                            steps: Optional[List[Dict]] = None,
                            symbol: Optional[str] = None,
                            interval: Optional[str] = None) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –≤ –∫–æ–Ω–≤–µ—î—Ä—ñ")
            return data

        if steps is None:
            steps = [
                {'name': 'remove_duplicate_timestamps', 'params': {}},
                {'name': 'clean_data', 'params': {'remove_outliers': True, 'fill_missing': True}},
                {'name': 'handle_missing_values', 'params': {
                    'method': 'interpolate',
                    'fetch_missing': True
                }}
            ]

        self.logger.info(f"–ü–æ—á–∞—Ç–æ–∫ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫–æ–Ω–≤–µ—î—Ä–∞ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö –∑ {len(steps)} –∫—Ä–æ–∫–∞–º–∏")
        result = data.copy()

        for step_idx, step in enumerate(steps, 1):
            step_name = step.get('name')
            step_params = step.get('params', {})

            if not hasattr(self, step_name):
                self.logger.warning(f"–ö—Ä–æ–∫ {step_idx}: –ú–µ—Ç–æ–¥ '{step_name}' –Ω–µ —ñ—Å–Ω—É—î. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
                continue

            try:
                self.logger.info(f"–ö—Ä–æ–∫ {step_idx}: –í–∏–∫–æ–Ω–∞–Ω–Ω—è '{step_name}' –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ {step_params}")
                method = getattr(self, step_name)

                # –î–æ–¥–∞—î–º–æ symbol —Ç–∞ interval —è–∫—â–æ –º–µ—Ç–æ–¥ –ø—ñ–¥—Ç—Ä–∏–º—É—î —ó—Ö
                if step_name == 'handle_missing_values':
                    step_params['symbol'] = symbol
                    step_params['interval'] = interval

                if step_name == 'normalize_data':
                    result, _ = method(result, **step_params)
                elif step_name == 'detect_outliers':
                    outliers_df, _ = method(result, **step_params)
                    self.logger.info(f"–í–∏—è–≤–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª—ñ—ó, –∞–ª–µ –¥–∞–Ω—ñ –Ω–µ –∑–º—ñ–Ω–µ–Ω–æ")
                else:
                    result = method(result, **step_params)

                self.logger.info(
                    f"–ö—Ä–æ–∫ {step_idx}: '{step_name}' –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(result)} —Ä—è–¥–∫—ñ–≤, {len(result.columns)} –∫–æ–ª–æ–Ω–æ–∫")

            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞ –∫—Ä–æ—Ü—ñ {step_idx}: '{step_name}': {str(e)}")

        self.logger.info(
            f"–ö–æ–Ω–≤–µ—î—Ä –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü–æ—á–∞—Ç–∫–æ–≤–æ: {len(data)} —Ä—è–¥–∫—ñ–≤, {len(data.columns)} –∫–æ–ª–æ–Ω–æ–∫. "
            f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {len(result)} —Ä—è–¥–∫—ñ–≤, {len(result.columns)} –∫–æ–ª–æ–Ω–æ–∫.")

        return result


def main():
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    EU_TIMEZONE = 'Europe/Kiev'
    SYMBOLS = ['BTC', 'ETH', 'SOL']
    INTERVALS = ['1d', '1h', '4h']

    data_source_paths = {
        'csv': {
            'BTC': {
                '1d': '/Users/bogdanresetko/Desktop/kursova/data/crypto_data/BTCUSDT_1d.csv',
                '1h': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/BTCUSDT_1h.csv',
                '4h': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/BTCUSDT_4h.csv'
            },
            'ETH': {
                '1d': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/ETHUSDT_1d.csv',
                '1h': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/ETHUSDT_1h.csv',
                '4h': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/ETHUSDT_4h.csv'
            },
            'SOL': {
                '1d': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/SOLUSDT_1d.csv',
                '1h': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/SOLUSDT_1h.csv',
                '4h': '/Users/bogdanresetko/Desktop/kursova//data/crypto_data/SOLUSDT_4h.csv'
            }
        }
    }

    processor = MarketDataProcessor()

    for symbol in SYMBOLS:
        for interval in INTERVALS:
            print(f"\nüîÑ –û–±—Ä–æ–±–∫–∞ {symbol} {interval}...")

            data = processor.load_data(
                data_source='database',
                symbol=symbol,
                interval=interval,
                data_type='candles'
            )

            if data.empty:
                file_path = data_source_paths['csv'].get(symbol, {}).get(interval)
                if not file_path:
                    print(f"‚ö†Ô∏è –ù–µ–º–∞—î CSV-—Ñ–∞–π–ª—É –¥–ª—è {symbol} {interval}")
                    continue

                print(f"üìÅ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ CSV: {file_path}")
                data = processor.load_data(
                    data_source='csv',
                    symbol=symbol,
                    interval=interval,
                    file_path=file_path,
                    data_type='candles'
                )

                if data.empty:
                    print(f"‚ö†Ô∏è –î–∞–Ω—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥–ª—è {symbol} {interval}")
                    continue

                processor.save_klines_to_db(data, symbol, interval)
                print("üì• –ó–±–µ—Ä–µ–∂–µ–Ω–æ —Å–≤—ñ—á–∫–∏ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö")

            print(f"‚úîÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(data)} —Ä—è–¥–∫—ñ–≤")

            # –û–±—Ä–æ–±–∫–∞
            processed_data = processor.preprocess_pipeline(
                data,
                symbol=symbol,
                interval=interval
            )

            if interval != '1d':
                processed_data = processor.resample_data(processed_data, target_interval='1d')

            processed_data = processor.add_time_features(processed_data, tz=EU_TIMEZONE)

            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            processor.save_processed_klines_to_db(processed_data, symbol, '1d')
            print("‚úÖ –û–±—Ä–æ–±–ª–µ–Ω—ñ –¥–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –ë–î")

            # –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é –æ–±'—î–º—É
            volume_profile = processor.aggregate_volume_profile(
                processed_data, bins=12, time_period='1W'
            )
            if not volume_profile.empty:
                processor.save_volume_profile_to_db(volume_profile, symbol, '1d')
                print("üìä –ü—Ä–æ—Ñ—ñ–ª—å –æ–±'—î–º—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ")

if __name__ == "__main__":
    main()
