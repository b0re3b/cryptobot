from datetime import datetime
from typing import List, Tuple, Dict, Optional, Union
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from data_collection.AnomalyDetector import AnomalyDetector
from data_collection.DataResampler import DataResampler
from utils.config import BINANCE_API_KEY, BINANCE_API_SECRET


class DataCleaner:
    def __init__(self, logger, db_manager):
        self.logger = logger
        self.db_manager = db_manager


    def clean_data(self, data: pd.DataFrame, remove_outliers: bool = True,
                   fill_missing: bool = True, normalize: bool = False,
                   norm_method: str = 'z-score', resample: bool = False,
                   target_interval: str = None, add_time_features: bool = False,
                   cyclical: bool = True, add_sessions: bool = False) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è")
            return data

        self.logger.info(f"–ü–æ—á–∞—Ç–æ–∫ –æ—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {data.shape[0]} —Ä—è–¥–∫—ñ–≤, {data.shape[1]} —Å—Ç–æ–≤–ø—Ü—ñ–≤")

        # –í–∏–∫–æ–Ω—É—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —Ü—ñ–ª—ñ—Å–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–¥ –æ—á–∏—â–µ–Ω–Ω—è–º
        integrity_issues = AnomalyDetector.validate_data_integrity(data)  # –í–∏–∫–ª–∏–∫–∞—î–º–æ –º–µ—Ç–æ–¥ –∑ db_manager
        if integrity_issues:
            issue_count = sum(len(issues) if isinstance(issues, list) or isinstance(issues, dict) else 1
                              for issues in integrity_issues.values())
            self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {issue_count} –ø—Ä–æ–±–ª–µ–º –∑ —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö")

        result = data.copy()

        if not isinstance(result.index, pd.DatetimeIndex):
            try:
                time_cols = [col for col in result.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    self.logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ {time_col} –≤ —ñ–Ω–¥–µ–∫—Å —á–∞—Å—É")
                    result[time_col] = pd.to_datetime(result[time_col])
                    result.set_index(time_col, inplace=True)
                else:
                    self.logger.warning("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–∫—É –∑ —á–∞—Å–æ–º, —ñ–Ω–¥–µ–∫—Å –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –Ω–µ–∑–º—ñ–Ω–Ω–∏–º")
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞–Ω–Ω—ñ —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")

        if result.index.duplicated().any():
            dup_count = result.index.duplicated().sum()
            self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {dup_count} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —ñ–Ω–¥–µ–∫—Å—É, –≤–∏–¥–∞–ª–µ–Ω–Ω—è...")
            result = result[~result.index.duplicated(keep='first')]

        result = result.sort_index()

        # –î–æ–¥–∞—î–º–æ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if add_time_features:
            self.logger.info("–î–æ–¥–∞–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫...")
            result = self.add_time_features(
                data=result,
                cyclical=cyclical,
                add_sessions=add_sessions
            )

        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in result.columns:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–µ —î –∫–æ–ª–æ–Ω–∫–∞ –≤–∂–µ —á–∏—Å–ª–æ–≤–æ–≥–æ —Ç–∏–ø—É
                if not pd.api.types.is_numeric_dtype(result[col]):
                    self.logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ {col} –≤ —á–∏—Å–ª–æ–≤–∏–π —Ç–∏–ø")
                    result[col] = pd.to_numeric(result[col], errors='coerce')
                    result[col] = result[col].astype(float)

        if remove_outliers:
            self.logger.info("–í–∏–¥–∞–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å...")
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
            for col in price_cols:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –∞–±–æ —Å–µ—Ä—ñ—é
                if result[col].empty or result[col].isna().all():
                    continue

                Q1 = result[col].quantile(0.25)
                Q3 = result[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 3 * IQR
                upper_bound = Q3 + 3 * IQR

                outliers = (result[col] < lower_bound) | (result[col] > upper_bound)
                if outliers.any():
                    outlier_count = outliers.sum()
                    outlier_indexes = result.index[outliers].tolist()
                    self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {outlier_count} –∞–Ω–æ–º–∞–ª—ñ–π –≤ –∫–æ–ª–æ–Ω—Ü—ñ {col}")
                    self.logger.debug(
                        f"–Ü–Ω–¥–µ–∫—Å–∏ –ø–µ—Ä—à–∏—Ö 10 –∞–Ω–æ–º–∞–ª—ñ–π: {outlier_indexes[:10]}{'...' if len(outlier_indexes) > 10 else ''}")
                    result.loc[outliers, col] = np.nan

        if fill_missing and result.isna().any().any():
            self.logger.info("–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å...")
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
            if price_cols and isinstance(result.index, pd.DatetimeIndex):
                result[price_cols] = result[price_cols].interpolate(method='time')

            if 'volume' in result.columns and result['volume'].isna().any():
                result['volume'] = result['volume'].fillna(0)

            numeric_cols = result.select_dtypes(include=[np.number]).columns
            other_numeric = [col for col in numeric_cols if col not in price_cols + ['volume']]
            if other_numeric:
                if isinstance(result.index, pd.DatetimeIndex):
                    result[other_numeric] = result[other_numeric].interpolate(method='time')
                else:
                    result[other_numeric] = result[other_numeric].interpolate(method='linear')

            result = result.ffill().bfill()

        price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
        if len(price_cols) == 4:
            invalid_hl = result['high'] < result['low']
            if invalid_hl.any():
                invalid_count = invalid_hl.sum()
                invalid_indexes = result.index[invalid_hl].tolist()
                self.logger.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {invalid_count} —Ä—è–¥–∫—ñ–≤, –¥–µ high < low")
                self.logger.debug(
                    f"–Ü–Ω–¥–µ–∫—Å–∏ –ø—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤: {invalid_indexes[:10]}{'...' if len(invalid_indexes) > 10 else ''}")

                temp = result.loc[invalid_hl, 'high'].copy()
                result.loc[invalid_hl, 'high'] = result.loc[invalid_hl, 'low']
                result.loc[invalid_hl, 'low'] = temp

        # –í–∏–∫–æ–Ω—É—î–º–æ —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥ –¥–∞–Ω–∏—Ö, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if resample and target_interval:
            try:
                self.logger.info(f"–í–∏–∫–æ–Ω–∞–Ω–Ω—è —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥—É –¥–∞–Ω–∏—Ö –¥–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É {target_interval}...")
                result = DataResampler.resample_data(result, target_interval)
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥—É –¥–∞–Ω–∏—Ö: {str(e)}")

        # –î–æ–¥–∞—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é –¥–∞–Ω–∏—Ö, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if normalize:
            self.logger.info(f"–í–∏–∫–æ–Ω–∞–Ω–Ω—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö –º–µ—Ç–æ–¥–æ–º {norm_method}...")
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]

            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ü—ñ–Ω–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            if price_cols:
                result, price_scaler = self.normalize_data(
                    data=result,
                    method=norm_method,
                    columns=price_cols
                )

                if price_scaler is None:
                    self.logger.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ü—ñ–Ω–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏")

            # –û–∫—Ä–µ–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –æ–±'—î–º, —è–∫—â–æ –≤—ñ–Ω –ø—Ä–∏—Å—É—Ç–Ω—ñ–π
            if 'volume' in result.columns:
                result, volume_scaler = self.normalize_data(
                    data=result,
                    method='min-max',  # –î–ª—è –æ–±'—î–º—É –∫—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ min-max –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é
                    columns=['volume']
                )

                if volume_scaler is None:
                    self.logger.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∫–æ–ª–æ–Ω–∫—É –æ–±'—î–º—É")

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –ø—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è
        clean_integrity_issues = AnomalyDetector.validate_data_integrity(result)
        if clean_integrity_issues:
            issue_count = sum(len(issues) if isinstance(issues, list) or isinstance(issues, dict) else 1
                              for issues in clean_integrity_issues.values())
            self.logger.info(f"–ü—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è –∑–∞–ª–∏—à–∏–ª–æ—Å—å {issue_count} –ø—Ä–æ–±–ª–µ–º –∑ —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö")

        self.logger.info(f"–û—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {result.shape[0]} —Ä—è–¥–∫—ñ–≤, {result.shape[1]} —Å—Ç–æ–≤–ø—Ü—ñ–≤")
        return result

    def handle_missing_values(self, data: pd.DataFrame, method: str = 'interpolate',
                              symbol: str = None, interval: str = None,
                              fetch_missing: bool = True) -> pd.DataFrame:
        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å")
            return data


        integrity_issues = AnomalyDetector.validate_data_integrity(data)
        if integrity_issues:
            issue_count = sum(len(issues) if isinstance(issues, list) or isinstance(issues, dict) else 1
                              for issues in integrity_issues.values())
            self.logger.warning(f"–ü–µ—Ä–µ–¥ –æ–±—Ä–æ–±–∫–æ—é –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –∑–Ω–∞–π–¥–µ–Ω–æ {issue_count} –ø—Ä–æ–±–ª–µ–º –∑ —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö")
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –ø—Ä–æ–±–ª–µ–º–∏ –∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ —Å–µ—Ä–µ–¥ –≤–∏—è–≤–ª–µ–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º
            if "columns_with_na" in integrity_issues:
                na_cols = list(integrity_issues["columns_with_na"].keys())
                self.logger.info(f"–í–∏—è–≤–ª–µ–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ –∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏: {na_cols}")

        result = data.copy()
        missing_values = result.isna().sum()
        total_missing = missing_values.sum()

        if total_missing == 0:
            self.logger.info("–í—ñ–¥—Å—É—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return result

        self.logger.info(
            f"–ó–Ω–∞–π–¥–µ–Ω–æ {total_missing} –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å —É {len(missing_values[missing_values > 0])} –∫–æ–ª–æ–Ω–∫–∞—Ö")

        # üîÑ –ü—ñ–¥—Ç—è–≥—É–≤–∞–Ω–Ω—è –∑ Binance
        if isinstance(result.index, pd.DatetimeIndex) and fetch_missing:
            time_diff = result.index.to_series().diff()
            expected_diff = time_diff.dropna().median() if len(time_diff) > 5 else None

            if expected_diff and symbol and interval:
                missing_periods = self._detect_missing_periods(result, expected_diff)
                if missing_periods:
                    self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(missing_periods)} –ø—Ä–æ–≥–∞–ª–∏–Ω. –ü—ñ–¥—Ç—è–≥—É—î–º–æ –∑ Binance...")
                    filled = self._fetch_missing_data_from_binance(result, missing_periods, symbol, interval)
                    result = pd.concat([result, filled])
                    result = result[~result.index.duplicated(keep='last')].sort_index()

        filled_values = 0
        numeric_cols = result.select_dtypes(include=[np.number]).columns

        if method == 'interpolate':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É –ª—ñ–Ω—ñ–π–Ω–æ—ó —ñ–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—ó")
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in result.columns]
            other_cols = [col for col in numeric_cols if col not in price_cols]
            before_fill = result.count().sum()

            if price_cols:
                result[price_cols] = result[price_cols].interpolate(method='time')

            if other_cols:
                result[other_cols] = result[other_cols].interpolate().ffill().bfill()

            result = result.ffill().bfill()
            filled_values = result.count().sum() - before_fill

        elif method == 'ffill':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É forward/backward fill")
            before_fill = result.count().sum()
            result = result.ffill().bfill()
            filled_values = result.count().sum() - before_fill

        elif method == 'mean':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—ñ–º –∑–Ω–∞—á–µ–Ω–Ω—è–º")
            for col in numeric_cols:
                if missing_values[col] > 0:
                    if result[col].dropna().empty:
                        self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∑–Ω–∞—á–µ–Ω—å –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ")
                        continue
                    col_mean = result[col].mean()
                    if pd.notna(col_mean):
                        before = result[col].isna().sum()
                        result[col] = result[col].fillna(col_mean)
                        filled_values += before - result[col].isna().sum()

        elif method == 'median':
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –º–µ–¥—ñ–∞–Ω–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º")
            for col in numeric_cols:
                if missing_values[col] > 0:
                    if result[col].dropna().empty:
                        self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∑–Ω–∞—á–µ–Ω—å –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ–¥—ñ–∞–Ω–∏")
                        continue
                    col_median = result[col].median()
                    if pd.notna(col_median):
                        before = result[col].isna().sum()
                        result[col] = result[col].fillna(col_median)
                        filled_values += before - result[col].isna().sum()

        else:
            self.logger.warning(f"–ú–µ—Ç–æ–¥ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è '{method}' –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è")

        remaining_missing = result.isna().sum().sum()
        if remaining_missing > 0:
            self.logger.warning(f"–ó–∞–ª–∏—à–∏–ª–æ—Å—è {remaining_missing} –Ω–µ–∑–∞–ø–æ–≤–Ω–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –ø—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏")

        self.logger.info(f"–ó–∞–ø–æ–≤–Ω–µ–Ω–æ {filled_values} –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –º–µ—Ç–æ–¥–æ–º '{method}'")


        clean_integrity_issues = AnomalyDetector.validate_data_integrity(result)
        if clean_integrity_issues:
            issue_count = sum(len(issues) if isinstance(issues, list) or isinstance(issues, dict) else 1
                              for issues in clean_integrity_issues.values())
            self.logger.info(f"–ü—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –∑–∞–ª–∏—à–∏–ª–æ—Å—å {issue_count} –ø—Ä–æ–±–ª–µ–º –∑ —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö")

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –∑–∞–ª–∏—à–∏–ª–∏—Å—å –ø—Ä–æ–±–ª–µ–º–∏ –∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
            if "columns_with_na" in clean_integrity_issues:
                na_cols = list(clean_integrity_issues["columns_with_na"].keys())
                self.logger.warning(f"–ü—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏ –≤—Å–µ —â–µ —î –∫–æ–ª–æ–Ω–∫–∏ –∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏: {na_cols}")
        else:
            self.logger.info("–ü—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –ø—Ä–æ–±–ª–µ–º –∑ —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ")

        return result

    def _detect_missing_periods(self, data: pd.DataFrame, expected_diff: pd.Timedelta) -> List[
        Tuple[datetime, datetime]]:

        if not isinstance(data.index, pd.DatetimeIndex) or data.empty:
            return []

        if expected_diff is None:
            self.logger.warning("expected_diff —î None, –Ω–µ–º–æ–∂–ª–∏–≤–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ñ –ø–µ—Ä—ñ–æ–¥–∏")
            return []

        sorted_index = data.index.sort_values()

        time_diff = sorted_index.to_series().diff()
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±—ñ–ª—å—à –±–µ–∑–ø–µ—á–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        large_gaps = time_diff[time_diff > expected_diff * 1.5]

        missing_periods = []
        for timestamp, gap in large_gaps.items():
            prev_timestamp = timestamp - gap

            # –ó–∞–ø–æ–±—ñ–≥–∞—î–º–æ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ–º—É –ø–µ—Ä–µ–ø–æ–≤–Ω–µ–Ω–Ω—é –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ missing_steps
            try:
                missing_steps = max(0, int(gap / expected_diff) - 1)
                if missing_steps > 0:
                    self.logger.info(
                        f"–í–∏—è–≤–ª–µ–Ω–æ –ø—Ä–æ–º—ñ–∂–æ–∫: {prev_timestamp} - {timestamp} ({missing_steps} –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤)")
                    missing_periods.append((prev_timestamp, timestamp))
            except (OverflowError, ZeroDivisionError) as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ missing_steps: {str(e)}")

        return missing_periods

    def _fetch_missing_data_from_binance(self, data: pd.DataFrame,
                                         missing_periods: List[Tuple[datetime, datetime]],
                                         symbol: str, interval: str) -> pd.DataFrame:
        if data is None or data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –¥–∞–Ω–∏–º–∏")
            return pd.DataFrame()

        if not symbol or not interval:
            self.logger.error("–ù–µ–≤–∞–ª—ñ–¥–Ω–∏–π symbol –∞–±–æ interval")
            return data

        try:
            from binance.client import Client
            api_key = BINANCE_API_KEY
            api_secret = BINANCE_API_SECRET

            if not api_key or not api_secret:
                self.logger.error("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–ª—é—á—ñ API Binance")
                return data

            client = Client(api_key, api_secret)
            filled_data = data.copy()

            valid_intervals = ['1m', '1h', '4h', '1d']
            if interval not in valid_intervals:
                self.logger.error(f"–ù–µ–≤–∞–ª—ñ–¥–Ω–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª: {interval}")
                return data

            new_data_frames = []

            for start_time, end_time in missing_periods:
                try:
                    self.logger.info(f" –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Binance: {symbol}, {interval}, {start_time} - {end_time}")
                    start_ms = int(start_time.timestamp() * 1000)
                    end_ms = int(end_time.timestamp() * 1000)
                    self.logger.info(f"–ó–∞–ø–∏—Ç –¥–æ Binance: {start_time} -> {start_ms} –º—Å, {end_time} -> {end_ms} –º—Å")

                    klines = client.get_historical_klines(
                        symbol=symbol,
                        interval=interval,
                        start_str=start_ms,
                        end_str=end_ms
                    )

                    if not klines:
                        self.logger.warning(f" –ü–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ Binance: {start_time} - {end_time}")
                        continue

                    columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume',
                               'close_time', 'quote_asset_volume', 'number_of_trades',
                               'taker_buy_base_volume', 'taker_buy_quote_volume', 'ignore']
                    binance_df = pd.DataFrame(klines, columns=columns[:len(klines[0])])

                    binance_df['timestamp'] = pd.to_datetime(binance_df['timestamp'], unit='ms')
                    binance_df.set_index('timestamp', inplace=True)

                    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —á–∏—Å–ª–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
                    for col in ['open', 'high', 'low', 'close', 'volume']:
                        if col in binance_df.columns:
                            binance_df[col] = pd.to_numeric(binance_df[col], errors='coerce')

                    binance_df['is_closed'] = True

                    # –í–∏–±–∏—Ä–∞—î–º–æ –ª–∏—à–µ —Ç—ñ –∫–æ–ª–æ–Ω–∫–∏, —è–∫—ñ —î –≤ –æ–±–æ—Ö DataFrame
                    common_cols = data.columns.intersection(binance_df.columns)
                    if common_cols.empty:
                        self.logger.warning("‚ö†Ô∏è –ù–µ–º–∞—î —Å–ø—ñ–ª—å–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è")
                        continue

                    new_data = binance_df[common_cols]
                    new_data_frames.append(new_data)

                    self.logger.info(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(new_data)} –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")

                except Exception as e:
                    self.logger.error(f" –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ Binance: {e}")

            if not new_data_frames:
                return data

            combined_new = pd.concat(new_data_frames)
            total_before = len(filled_data)
            filled_data = pd.concat([filled_data, combined_new])
            filled_data = filled_data[~filled_data.index.duplicated(keep='last')]
            filled_data = filled_data.sort_index()
            total_after = len(filled_data)

            added_count = total_after - total_before
            self.logger.info(f" –ó–∞–≥–∞–ª–æ–º –¥–æ–¥–∞–Ω–æ {added_count} –Ω–æ–≤–∏—Ö —Ä—è–¥–∫—ñ–≤ –ø—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è")

            return filled_data

        except ImportError:
            self.logger.error(" –ú–æ–¥—É–ª—å binance –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")
            return data

    def normalize_data(self, data: pd.DataFrame, method: str = 'z-score',
                       columns: List[str] = None, exclude_columns: List[str] = None) -> Tuple[pd.DataFrame, Dict]:

        if data is None or data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            return data, None

        result = data.copy()

        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

        if columns is not None:
            normalize_cols = [col for col in columns if col in numeric_cols]
            if not normalize_cols:
                self.logger.warning("–ñ–æ–¥–Ω–∞ –∑ —É–∫–∞–∑–∞–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ —î —á–∏—Å–ª–æ–≤–æ—é")
                return result, None
        else:
            normalize_cols = numeric_cols

        if exclude_columns is not None:
            normalize_cols = [col for col in normalize_cols if col not in exclude_columns]

        if not normalize_cols:
            self.logger.warning("–ù–µ–º–∞—î —á–∏—Å–ª–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            return result, None

        self.logger.info(f"–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è {len(normalize_cols)} –∫–æ–ª–æ–Ω–æ–∫ –º–µ—Ç–æ–¥–æ–º {method}")

        X = result[normalize_cols].values

        scaler = None
        if method == 'z-score':
            scaler = StandardScaler()
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è StandardScaler (z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)")
        elif method == 'min-max':
            scaler = MinMaxScaler()
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è MinMaxScaler (min-max –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)")
        elif method == 'robust':
            scaler = RobustScaler()
            self.logger.info("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è RobustScaler (—Ä–æ–±–∞—Å—Ç–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)")
        else:
            self.logger.error(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó: {method}")
            return result, None

        try:
            # –û–±—Ä–æ–±–∫–∞ NaN –∑–Ω–∞—á–µ–Ω—å
            if np.isnan(X).any():
                self.logger.warning("–ó–Ω–∞–π–¥–µ–Ω–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è –≤ –¥–∞–Ω–∏—Ö. –ó–∞–º—ñ–Ω–∞ –Ω–∞ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫")
                for i, col in enumerate(normalize_cols):
                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ –≤—Å—è –∫–æ–ª–æ–Ω–∫–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ NaN
                    if np.all(np.isnan(X[:, i])):
                        self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –º—ñ—Å—Ç–∏—Ç—å –ª–∏—à–µ NaN –∑–Ω–∞—á–µ–Ω–Ω—è. –ó–∞–º—ñ–Ω–∞ –Ω–∞ 0.")
                        X[:, i] = 0
                    else:
                        col_mean = np.nanmean(X[:, i])
                        X[:, i] = np.nan_to_num(X[:, i], nan=col_mean)

            X_scaled = scaler.fit_transform(X)

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
            if not np.isfinite(X_scaled).all():
                self.logger.warning("–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó. –ó–∞–º—ñ–Ω–∞ –Ω–∞ 0.")
                X_scaled = np.nan_to_num(X_scaled, nan=0, posinf=0, neginf=0)

            for i, col in enumerate(normalize_cols):
                result[col] = X_scaled[:, i]

            self.logger.info(f"–£—Å–ø—ñ—à–Ω–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∫–æ–ª–æ–Ω–∫–∏: {normalize_cols}")

            scaler_meta = {
                'method': method,
                'columns': normalize_cols,
                'scaler': scaler
            }

            return result, scaler_meta

        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö: {str(e)}")
            return data, None

    def add_time_features(self, data: pd.DataFrame, cyclical: bool = True,
                          add_sessions: bool = False, tz: str = 'Europe/Kiev') -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")
            return data

        result = data.copy()

        if not isinstance(result.index, pd.DatetimeIndex):
            self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
            try:
                time_cols = [col for col in result.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    result[time_col] = pd.to_datetime(result[time_col])
                    result.set_index(time_col, inplace=True)
                else:
                    self.logger.error("–ù–µ–º–æ–∂–ª–∏–≤–æ –¥–æ–¥–∞—Ç–∏ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É")
                    return data
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")
                return data

        self.logger.info("–î–æ–¥–∞–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")

        if result.index.tz is None:
            self.logger.info(f"–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É {tz}")
            try:
                result.index = result.index.tz_localize(tz)
            except Exception as e:
                self.logger.warning(
                    f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É: {str(e)}. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É.")
        elif result.index.tz.zone != tz:
            self.logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É –∑ {result.index.tz.zone} –≤ {tz}")
            try:
                result.index = result.index.tz_convert(tz)
            except Exception as e:
                self.logger.warning(
                    f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å—É: {str(e)}. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ –ø–æ—Ç–æ—á–Ω–∏–º —á–∞—Å–æ–≤–∏–º –ø–æ—è—Å–æ–º.")

        result['hour'] = result.index.hour
        result['day'] = result.index.day
        result['weekday'] = result.index.weekday
        result['week'] = result.index.isocalendar().week
        result['month'] = result.index.month
        result['quarter'] = result.index.quarter
        result['year'] = result.index.year
        result['dayofyear'] = result.index.dayofyear

        result['is_weekend'] = result['weekday'].isin([5, 6]).astype(int)
        result['is_month_start'] = result.index.is_month_start.astype(int)
        result['is_month_end'] = result.index.is_month_end.astype(int)
        result['is_quarter_start'] = result.index.is_quarter_start.astype(int)
        result['is_quarter_end'] = result.index.is_quarter_end.astype(int)
        result['is_year_start'] = result.index.is_year_start.astype(int)
        result['is_year_end'] = result.index.is_year_end.astype(int)

        if cyclical:
            self.logger.info("–î–æ–¥–∞–≤–∞–Ω–Ω—è —Ü–∏–∫–ª—ñ—á–Ω–∏—Ö –æ–∑–Ω–∞–∫")

            result['hour_sin'] = np.sin(2 * np.pi * result['hour'] / 24)
            result['hour_cos'] = np.cos(2 * np.pi * result['hour'] / 24)

            days_in_month = result.index.days_in_month
            result['day_sin'] = np.sin(2 * np.pi * result['day'] / days_in_month)
            result['day_cos'] = np.cos(2 * np.pi * result['day'] / days_in_month)

            result['weekday_sin'] = np.sin(2 * np.pi * result['weekday'] / 7)
            result['weekday_cos'] = np.cos(2 * np.pi * result['weekday'] / 7)

            result['week_sin'] = np.sin(2 * np.pi * result['week'] / 52)
            result['week_cos'] = np.cos(2 * np.pi * result['week'] / 52)

            result['month_sin'] = np.sin(2 * np.pi * result['month'] / 12)
            result['month_cos'] = np.cos(2 * np.pi * result['month'] / 12)

            result['quarter_sin'] = np.sin(2 * np.pi * result['quarter'] / 4)
            result['quarter_cos'] = np.cos(2 * np.pi * result['quarter'] / 4)

        if add_sessions:
            self.logger.info("–î–æ–¥–∞–≤–∞–Ω–Ω—è —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ —Ç–æ—Ä–≥–æ–≤–∏—Ö —Å–µ—Å—ñ–π")

            result['asian_session'] = ((result['hour'] >= 0) & (result['hour'] < 9)).astype(int)

            result['european_session'] = ((result['hour'] >= 8) & (result['hour'] < 17)).astype(int)

            result['american_session'] = ((result['hour'] >= 13) & (result['hour'] < 22)).astype(int)

            result['asia_europe_overlap'] = ((result['hour'] >= 8) & (result['hour'] < 9)).astype(int)
            result['europe_america_overlap'] = ((result['hour'] >= 13) & (result['hour'] < 17)).astype(int)

            result['inactive_hours'] = ((result['hour'] >= 22) | (result['hour'] < 0)).astype(int)

        self.logger.info(f"–£—Å–ø—ñ—à–Ω–æ –¥–æ–¥–∞–Ω–æ {len(result.columns) - len(data.columns)} —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")
        return result

    def remove_duplicate_timestamps(self, data: pd.DataFrame) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤")
            return data

        if not isinstance(data.index, pd.DatetimeIndex):
            self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
            try:
                time_cols = [col for col in data.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    data[time_col] = pd.to_datetime(data[time_col])
                    data.set_index(time_col, inplace=True)
                else:
                    self.logger.error("–ù–µ–º–æ–∂–ª–∏–≤–æ –≤–∏—è–≤–∏—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É")
                    return data
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")
                return data

        duplicates = data.index.duplicated()
        duplicates_count = duplicates.sum()

        if duplicates_count == 0:
            self.logger.info("–î—É–±–ª—ñ–∫–∞—Ç–∏ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return data

        self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {duplicates_count} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫")

        result = data[~duplicates]

        self.logger.info(f"–í–∏–¥–∞–ª–µ–Ω–æ {duplicates_count} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {len(result)} –∑–∞–ø–∏—Å—ñ–≤.")
        return result
    def filter_by_time_range(self, data: pd.DataFrame,
                             start_time: Optional[Union[str, datetime]] = None,
                             end_time: Optional[Union[str, datetime]] = None) -> pd.DataFrame:

        if data.empty:
            self.logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó")
            return data

        if not isinstance(data.index, pd.DatetimeIndex):
            self.logger.warning("–Ü–Ω–¥–µ–∫—Å –Ω–µ —î DatetimeIndex. –°–ø—Ä–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏.")
            try:
                time_cols = [col for col in data.columns if
                             any(x in col.lower() for x in ['time', 'date', 'timestamp'])]
                if time_cols:
                    time_col = time_cols[0]
                    data[time_col] = pd.to_datetime(data[time_col])
                    data.set_index(time_col, inplace=True)
                else:
                    self.logger.error("–ù–µ–º–æ–∂–ª–∏–≤–æ —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ –∑–∞ —á–∞—Å–æ–º: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É")
                    return data
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —ñ–Ω–¥–µ–∫—Å—É: {str(e)}")
                return data

        result = data.copy()
        initial_count = len(result)

        if start_time is not None:
            try:
                start_dt = pd.to_datetime(start_time)
                result = result[result.index >= start_dt]
                self.logger.info(f"–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –ø–æ—á–∞—Ç–∫–æ–≤–∏–º —á–∞—Å–æ–º: {start_dt}")
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó –ø–æ—á–∞—Ç–∫–æ–≤–æ–≥–æ —á–∞—Å—É: {str(e)}")

        if end_time is not None:
            try:
                end_dt = pd.to_datetime(end_time)
                result = result[result.index <= end_dt]
                self.logger.info(f"–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –∫—ñ–Ω—Ü–µ–≤–∏–º —á–∞—Å–æ–º: {end_dt}")
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó –∫—ñ–Ω—Ü–µ–≤–æ–≥–æ —á–∞—Å—É: {str(e)}")

        final_count = len(result)

        if start_time is not None and end_time is not None:
            start_dt = pd.to_datetime(start_time)
            end_dt = pd.to_datetime(end_time)
            if start_dt > end_dt:
                self.logger.warning(f"–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —á–∞—Å ({start_dt}) –ø—ñ–∑–Ω—ñ—à–µ –∫—ñ–Ω—Ü–µ–≤–æ–≥–æ —á–∞—Å—É ({end_dt})")

        self.logger.info(f"–í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {initial_count - final_count} –∑–∞–ø–∏—Å—ñ–≤. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {final_count} –∑–∞–ø–∏—Å—ñ–≤.")
        return result